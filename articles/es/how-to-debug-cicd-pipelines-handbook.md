```markdown
---
title: "C√≥mo depurar pipelines de CI/CD: Un manual sobre resoluci√≥n de problemas con herramientas de observabilidad"
date: 2025-06-20T16:34:01.032Z
author: Opaluwa Emidowojo
authorURL: https://www.freecodecamp.org/news/author/Tech-On-Diapers/
originalURL: https://www.freecodecamp.org/news/how-to-debug-cicd-pipelines-handbook/
posteditor: ""
proofreader: ""
---

La observabilidad es un cambio de juego para los pipelines de CI/CD, y es uno de los aspectos m√°s emocionantes de DevOps. Cuando comenc√© a trabajar con sistemas de CI/CD, asum√≠ que la parte m√°s dif√≠cil ser√≠a construir el pipeline. Pero con configuraciones cada vez m√°s complejas, el verdadero desaf√≠o es depurar fallos, como la ca√≠da de builds o fallos de pruebas solo en producci√≥n.

<!-- m√°s -->

Las herramientas de observabilidad, como registros, m√©tricas y trazas, brindan la visibilidad que necesitas para identificar problemas r√°pidamente. En este manual, exploraremos herramientas gratuitas y de c√≥digo abierto que puedes utilizar para hacer que tus pipelines de CI/CD sean m√°s confiables. Usaremos pasos pr√°cticos para resolver problemas como un profesional, sin necesidad de licencias empresariales.

## Tabla de contenidos

1.  [Requisitos previos][1]
    
2.  [Por qu√© es importante la observabilidad][2]
    
3.  [C√≥mo instalar y configurar Grafana Loki en infraestructura de bajo presupuesto][3]
    
4.  [C√≥mo implementar una alternativa a ELK Stack para la observabilidad de pipelines][4]
    
5.  [C√≥mo crear una estrategia de registro unificada entre los componentes del pipeline][5]
    
6.  [C√≥mo consultar y analizar registros para una resoluci√≥n de problemas efectiva][6]
    
7.  [C√≥mo configurar m√©tricas de Prometheus junto a tus registros][7]
    
8.  [C√≥mo crear dashboards de Grafana que combinen m√©tricas y registros][8]
    
9.  [C√≥mo usar ejemplares para pasar de m√©tricas a registros relevantes][9]
    
10.  [C√≥mo diagnosticar y arreglar problemas comunes de CI/CD][10]
    
11.  [C√≥mo implementar t√©cnicas avanzadas de depuraci√≥n][11]
    
12.  [C√≥mo realizar an√°lisis post-mortem efectivos usando registros][12]
    
13.  [C√≥mo optimizar el almacenamiento y gesti√≥n de registros][13]
    
14.  [Conclusi√≥n][14]
    

### Requisitos previos

Hay algunas cosas que deber√≠as saber y tener para sacar el m√°ximo provecho de este manual:

#### Conocimientos t√©cnicos:

-   Comprensi√≥n b√°sica de [pipelines de CI/CD][15] (por ejemplo, etapas de construcci√≥n, prueba, despliegue).
    
-   Familiaridad con [comandos de Linux/Unix][16] (por ejemplo, `mkdir`, `grep`, `curl`).
    
-   Comodidad con [conceptos b√°sicos de Docker][17] (por ejemplo, `docker run`, `docker-compose up`).
    
-   Opcional: Conocimiento de [conceptos de observabilidad][18] (registros, m√©tricas, trazas) o configuraci√≥n YAML.
    

#### Software y herramientas:

-   **Docker y Docker Compose**: Instalado y en funcionamiento (verifica con `docker --version` y `docker-compose --version`).
    
-   **Plataforma de CI/CD**: Acceso a GitHub Actions, Jenkins, o GitLab CI con un pipeline de muestra que genere registros.
    
-   **Editor de texto**: Para editar archivos YAML (por ejemplo, VS Code, Nano).
    
-   **Navegador web**: Para acceder a las interfaces de usuario de las herramientas (por ejemplo, Grafana en el puerto 3000, Kibana en 5601).
    
-   Opcional: `curl` para probar el reenv√≠o de registros, Git para el control de versiones.
    

#### Hardware e infraestructura:

-   M√°quina con:
    
    -   OS: Linux, Windows (con WSL2), o macOS.
        
    -   4GB RAM (8GB recomendados), 20GB de espacio libre en disco.
        
    -   Internet estable y habilidad para abrir puertos (por ejemplo, 3100 para Loki, 9200 para Elasticsearch).
        
-   Opcional: Acceso a proveedor de nube (por ejemplo, AWS, GCP) para configuraciones escalables.
    

#### Acceso y permisos:

-   Acceso de administrador para instalar Docker y configurar herramientas de CI/CD.
    
-   Permisos para modificar configuraciones de pipeline (por ejemplo, `.github/workflows`, `.gitlab-ci.yml`).
    
-   Opcional: Acceso a un registro de contenedores (por ejemplo, Docker Hub) para im√°genes personalizadas.
    

## **Por qu√© es importante la observabilidad**

Los pipelines modernos de CI/CD ya no son scripts lineales; ahora son sistemas complejos y distribuidos que involucran m√∫ltiples herramientas, entornos y capas de infraestructura. Un trabajo se ejecuta en GitHub Actions, otro se despliega a trav√©s de Jenkins, y un tercero construye im√°genes de Docker en un cl√∫ster de Kubernetes.

Entonces, cuando algo se rompe, terminas persiguiendo registros entre herramientas, adivinando d√≥nde se origin√≥ el problema y perdiendo horas tratando de reproducirlo.

Y lo que es peor, las herramientas tradicionales de depuraci√≥n a menudo se detienen en la superficie, mostrando solo trabajos fallidos sin el contexto de _por qu√©_ fallaron o _d√≥nde_ en el sistema se encuentra realmente la falla.

La observabilidad cambia el guion. En lugar de buscar entre registros desconectados o de repetir compilaciones fallidas a ciegas, la observabilidad te proporciona **visi√≥n**, no solo datos. Al combinar registros estructurados, m√©tricas y trazas, puedes:

-   Reconstruir exactamente qu√© sucedi√≥ en un fallo de pipeline
    
-   Rastrear un fallo a trav√©s de agentes de CI, pasos de despliegue y contenedores
    
-   Visualizar patrones y anomal√≠as antes de que se conviertan en interrupciones
    

M√°s importante a√∫n, la observabilidad te ayuda a **transitar de la depuraci√≥n reactiva a la prevenci√≥n proactiva**.

Esto es lo que aprender√°s y lograr√°s en esta gu√≠a:

-   Configurar una observabilidad rentable usando Grafana Loki, una alternativa ligera a ELK y OpenTelemetry
    
-   Crear una estrategia de registro unificada para conectar tu pipeline
    
-   Escribir consultas precisas para identificar r√°pidamente las causas ra√≠z, correlacionar registros, m√©tricas y trazas para una depuraci√≥n completa
    
-   Resolver problemas de CI/CD como fallos de compilaci√≥n, pruebas inconsistentes y ca√≠das de contenedores
    
-   Construir dashboards personalizados y herramientas diagn√≥sticas automatizadas
    
-   Promover la observabilidad a trav√©s de documentaci√≥n y post-mortems
```


### **C√≥mo Elegir la Herramienta de Observabilidad Correcta para CI/CD**

Aqu√≠ tienes una comparaci√≥n r√°pida de Grafana Loki, Lightweight ELK y Vector para la observabilidad en CI/CD:

| **Herramienta** | **Uso de Recursos** | **Complejidad de Configuraci√≥n** | **Mejor Para** | **Ajuste en CI/CD** |
| --- | --- | --- | --- | --- |
| **Grafana Loki** | Bajo (ligero) | F√°cil (basado en Docker) | Equipos peque√±os, infraestructura econ√≥mica | Tuber√≠as simples, registros JSON, usuarios de Grafana |
| **Lightweight ELK** | Alto (dependiente de Elasticsearch) | Moderada (multi-contenedor) | Equipos que necesitan b√∫squeda/visualizaci√≥n avanzada | Tuber√≠as complejas, necesidades de consulta rica |
| **Vector** | Muy bajo | F√°cil (binario √∫nico) | Configuraciones con recursos limitados | Configuraciones m√≠nimas, reenv√≠o de registros |

C√≥mo elegir:

-   **Loki**: Ideal para startups o desarrolladores solitarios con recursos limitados. Se integra bien con Prometheus/Grafana.
    
-   **ELK**: Lo mejor para equipos que necesitan las visualizaciones avanzadas de Kibana o manejar grandes vol√∫menes de registros.
    
-   **Vector**: Ideal para el reenv√≠o ligero de registros en configuraciones CI/CD distribuidas.
    

**Grafana Loki** es un sistema de agregaci√≥n de registros como ELK, pero es m√°s ligero, y es ideal para tuber√≠as CI/CD con infraestructuras limitadas.

## C√≥mo Instalar y Configurar Grafana Loki en Infraestructura Econ√≥mica

### üõ† Opci√≥n A: Configuraci√≥n R√°pida con Docker (Recomendada para Infraestructura Econ√≥mica)

1.  **Crea un directorio para la configuraci√≥n:**
    
    ```
     mkdir -p ~/loki-setup && cd ~/loki-setup
    ```
    
2.  **Crea un** `docker-compose.yml`:
    
    ```
     # Define una configuraci√≥n de Docker Compose para Grafana Loki y Promtail para agregar y raspar registros eficientemente.
     version: "3"
    
     services:
       loki:
         image: grafana/loki:2.9.4  # Utiliza la versi√≥n 2.9.4 de Loki para una agregaci√≥n de registros ligera.
         ports:
           - "3100:3100"  # Expone el puerto de la API HTTP de Loki para la ingesti√≥n y consultas de registros.
         command: -config.file=/etc/loki/loki-config.yaml  # Especifica el archivo de configuraci√≥n para Loki.
         volumes:
           - ./loki-config.yaml:/etc/loki/loki-config.yaml  # Monta el archivo de configuraci√≥n local en el contenedor.
    
       promtail:
         image: grafana/promtail:2.9.4  # Utiliza la versi√≥n 2.9.4 de Promtail para raspar y enviar registros a Loki.
         volumes:
           - /var/log:/var/log  # Monta el directorio de registros del anfitri√≥n para que Promtail lo raspe.
           - ./promtail-config.yaml:/etc/promtail/promtail-config.yaml  # Monta el archivo de configuraci√≥n de Promtail.
         command: -config.file=/etc/promtail/promtail-config.yaml  # Especifica el archivo de configuraci√≥n para Promtail.
    ```
    
3.  **Crea un b√°sico** `loki-config.yaml`:
    
    ```
     # Configura Grafana Loki para almacenamiento y consulta de registros ligeros en un entorno CI/CD.
     auth_enabled: false  # Desactiva la autenticaci√≥n para simplicidad (no recomendado para producci√≥n).
    
     server:
       http_listen_port: 3100  # Establece el puerto para la API HTTP de Loki.
    
     ingester:
       lifecycler:
         ring:
           kvstore:
             store: inmemory  # Usa almacenamiento en memoria para el anillo, adecuado para configuraciones peque√±as.
           replication_factor: 1  # Establece una sola r√©plica para un uso m√≠nimo de recursos.
       chunk_idle_period: 3m  # Descarga los fragmentos a almacenamiento tras 3 minutos de inactividad.
       max_chunk_age: 1h  # Retira fragmentos tras 1 hora para equilibrar rendimiento de almacenamiento y consulta.
    
     schema_config:
       configs:
         - from: 2023-01-01  # Define la fecha de inicio del esquema.
           store: boltdb-shipper  # Usa BoltDB para indexar registros.
           object_store: filesystem  # Almacena registros en el sistema de archivos local.
           schema: v11  # Especifica la versi√≥n del esquema para el almacenamiento de registros.
           index:
             prefix: index_  # Prefijo para los archivos de √≠ndice.
             period: 24h  # Rota √≠ndices diariamente.
    
     storage_config:
       boltdb_shipper:
         active_index_directory: /tmp/loki/index  # Directorio para los archivos de √≠ndice activos.
         cache_location: /tmp/loki/boltdb-cache  # Ubicaci√≥n de cach√© para BoltDB.
       filesystem:
         directory: /tmp/loki/chunks  # Directorio para almacenar fragmentos de registros.
    
     limits_config:
       enforce_metric_name: false  # Desactiva la aplicaci√≥n estricta de nombres de m√©tricas por flexibilidad.
    ```
    
4.  **Crea un b√°sico** `promtail-config.yaml`:
    
    ```
     # Configura Promtail para raspar registros del sistema y reenviarlos a Loki.
     server:
       http_listen_port: 9080  # Establece el puerto HTTP de Promtail para m√©tricas y verificaciones de salud.
       grpc_listen_port: 0  # Desactiva gRPC para reducir el uso de recursos.
    
     positions:
       filename: /tmp/positions.yaml  # Almacena la posici√≥n de los registros raspados para reanudar tras reinicios.
    
     clients:
       - url: http://loki:3100/loki/api/v1/push  # Especifica el punto final de Loki para la ingesti√≥n de registros.
    
     scrape_configs:
       - job_name: system  # Define un trabajo de raspado para registros del sistema.
         static_configs:
           - targets:
               - localhost  # Apunta al anfitri√≥n local para la colecci√≥n de registros.
             labels:
               job: varlogs  # Etiqueta los registros para una consulta f√°cil en Loki.
               __path__: /var/log/*.log  # Raspa todos los archivos de registro en el directorio /var/log.
    ```
    
5.  **Ejecutar:**
    
    ```
     # Inicia los contenedores de Loki y Promtail en modo separado para operaci√≥n en segundo plano.
     docker-compose up -d
    ```
    

```markdown
#### Solucionar Problemas de Configuraci√≥n de Loki

Si Loki o Promtail no se inician, uno de los siguientes puede ser el problema:

1.  **Fallos del contenedor**: Verifique los logs con `docker logs loki` o `docker logs promtail`. Busque errores como _‚Äúsin memoria‚Äù_ o _‚Äúpuerto ya en uso.‚Äù_
    
    -   Soluci√≥n: Aumente la memoria (por ejemplo, los l√≠mites de recursos en `docker-compose.yml`) o cambie los puertos (ej., `3101:3100`).
2.  **Logs no ingeridos**: Verifique que Promtail est√° raspando la ruta correcta (`/var/log/ci/*.log`) usando `docker exec promtail cat /etc/promtail/promtail-config.yaml`
    
    -   Soluci√≥n: Actualice `__path__` en `promtail-config.yaml` para que coincida con su directorio de logs de CI/CD.
3.  **Restricciones de Recursos**: Monitoree el uso de recursos con `docker stats` o `top` en el host.
    
    -   Soluci√≥n: Aseg√∫rese de que su m√°quina tenga al menos 4GB de RAM y 20GB de espacio en disco, seg√∫n se especifica en los prerrequisitos.

### Configuraci√≥n para el Registro de CI/CD

Para adaptarse a los logs de CI/CD, debe:

#### 1\. Configurar sus herramientas de CI/CD para escribir logs en el disco:

Por ejemplo, GitHub Actions con un corredor personalizado puede escribir logs en `/var/log/gha/*.log`.

Actualizar Promtail:

```
# Configura Promtail para raspar logs de corredores de GitHub Actions para la observabilidad de CI/CD.
scrape_configs:
  - job_name: github_actions  # Define un trabajo de raspeo para los logs de GitHub Actions.
    static_configs:
      - targets: ['localhost']  # Apunta al host local donde el corredor escribe los logs.
        labels:
          job: gha  # Etiqueta los logs para identificaci√≥n en consultas de Loki.
          __path__: /var/log/gha/*.log  # Raspa los logs del directorio especificado.
```

#### 2\. Usar registros estructurados (JSON):

Aseg√∫rese de que sus herramientas o scripts de CI/CD emitan logs en formato estructurado:

Ejemplo:

```
# Ejemplo de un log JSON estructurado para canalizaciones de CI/CD, que permite un f√°cil an√°lisis y consulta.
{
  "timestamp": "2025-05-10T13:00:00Z",  # Marca de tiempo UTC para la entrada del log.
  "level": "error",  # Nivel de log para indicar gravedad.
  "job": "deploy",  # Identifica el trabajo de CI/CD (ej., etapa de despliegue).
  "message": "Error al extraer imagen"  # Mensaje descriptivo para el error.
}
```

Esto ayuda al consultar con LogQL.

### C√≥mo Conectar Agentes de CI a Loki

Esta secci√≥n explica tres formas diferentes de llevar los logs de su canalizaci√≥n CI a Loki para monitoreo y an√°lisis:

#### Opci√≥n 1 ‚Äì Configuraci√≥n local:

Sus agentes de CI escriben archivos de logs en el disco, y Promtail (ejecut√°ndose en la misma m√°quina) lee esos archivos y los env√≠a a Loki.

#### Opci√≥n 2 ‚Äì Usando el controlador de logs de Docker (contenedores Docker):

Si sus agentes de CI se ejecutan en contenedores Docker, instale un complemento especial de Loki que captura autom√°ticamente toda la salida del contenedor y la env√≠a directamente a Loki sin necesitar archivos de logs separados.

```
# Instala el driver de logs de Docker para Loki para enviar logs de contenedores directamente a Loki.
docker plugin install grafana/loki-docker-driver:latest --alias loki --grant-all-permissions
```

Luego ejecute su contenedor de agente:

```
# Ejecuta un contenedor de agente CI con el driver de logs de Loki para reenviar logs.
docker run --log-driver=loki \
  --log-opt loki-url="http://<su-host-loki>:3100/loki/api/v1/push" \
  mi-imagen-agente-ci
```

#### Opci√≥n 3 ‚Äì Configuraci√≥n remota:

Si no puede instalar Promtail localmente, puede usar una herramienta de reenv√≠o de logs como [Fluent Bit][19] o [Vector][20] para recolectar logs y enviarlos a Loki a trav√©s de la red.

**El objetivo:** Independientemente de la opci√≥n que elija, terminar√° con todos sus logs de la canalizaci√≥n de CI centralizados en Loki, donde podr√° buscarlos, crear paneles en Grafana y configurar alertas cuando algo salga mal.

Esencialmente le da flexibilidad para integrar la colecci√≥n de logs seg√∫n la configuraci√≥n de su infraestructura: ya sea que prefiera agentes locales, complementos de Docker o reenv√≠o remoto.

## C√≥mo Implementar una Alternativa a ELK Stack para la Observabilidad de Canalizaciones

Cuando un ELK completo (Elasticsearch, Logstash, Kibana) es demasiado pesado para su infraestructura, puede optar por configuraciones livianas que logren una observabilidad similar a un costo y uso de recursos m√°s bajos.

### C√≥mo Instalar Versiones Livianas de Elasticsearch, Logstash y Kibana

Objetivo: Levantar un stack ELK m√≠nimo pero funcional para depurar canalizaciones de CI/CD.

#### 1\. Use Docker para iniciar contenedores livianos

Cree un `docker-compose.yml`:

```
# Define una configuraci√≥n de Docker Compose para un stack ELK liviano para agregar y visualizar logs de CI/CD.
version: '3.7'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0  # Usa Elasticsearch 7.17.0.
    container_name: elasticsearch
    environment:
      - discovery.type=single-node  # Ejecuta Elasticsearch en modo de nodo √∫nico para simplicidad.
      - xpack.security.enabled=false  # Desactiva funciones de seguridad para una configuraci√≥n liviana.
    ports:
      - "9200:9200"  # Expone el puerto API HTTP de Elasticsearch.
    volumes:
      - esdata:/usr/share/elasticsearch/data  # Persiste los datos de Elasticsearch.

  logstash:
    image: docker.elastic.co/logstash/logstash:7.17.0  # Usa Logstash 7.17.0.
    container_name: logstash
    ports:
      - "5044:5044"  # Puerto para recibir logs de Beats.
      - "9600:9600"  # Puerto para monitoreo de Logstash.
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf  # Monta el archivo de configuraci√≥n de Logstash.
```
```

vol√∫menes:
  esdata:  # Define un volumen para persistir datos de Elasticsearch.
```

#### 2\. Configuraci√≥n m√≠nima del pipeline de Logstash (logstash.conf)

```
// Configura Logstash para procesar y enviar los logs de CI/CD hacia Elasticsearch.
input {
  beats {
    port => 5044  // Escucha logs de Filebeat en el puerto 5044.
  }
}

filter {
  json {
    source => "message"  // Analiza los mensajes de log en formato JSON para datos estructurados.
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]  // Env√≠a los logs procesados a Elasticsearch.
    index => "ci-logs-%{+YYYY.MM.dd}"  // Almacena los logs en √≠ndices diarios (por ejemplo, ci-logs-2025.05.14).
  }
}
```

#### Soluci√≥n de problemas en la configuraci√≥n de ELK

Si Elasticsearch, Logstash o Kibana no se inician, uno de los siguientes podr√≠a ser el problema:

1.  **Fallo de contenedores**: Verifique los logs con `docker logs elasticsearch`, `docker logs logstash` o `docker logs kibana`. Busque errores como _‚Äúespacio en disco insuficiente‚Äù_ o _‚Äúconflicto de puertos‚Äù_ (por ejemplo, 9200, 5601).
    
    -   Soluci√≥n: Libere espacio en disco (aseg√∫rese de tener al menos 20GB disponibles) o cambie los puertos en `docker-compose.yml` (por ejemplo, `9201:9200`).
2.  **Logs no ingeridos**: Verifique que Logstash est√© recibiendo datos de Filebeat o Vector usando `docker logs logstash`. Verifique el puerto de entrada en `logstash.conf` (por ejemplo, 5044).
    
    -   Soluci√≥n: Aseg√∫rese de que Filebeat o Vector est√©n configurados para enviar al endpoint correcto de Logstash (por ejemplo, `localhost:5044`) y actualice si es necesario.
3.  **Restricciones de recursos**: Monitoree el uso de recursos con Docker stats o top en el host.
    
    -   Soluci√≥n: Asigne al menos 8GB de RAM y 30GB de espacio en disco, ya que Elasticsearch requiere m√°s recursos que Loki. Ajuste los l√≠mites de memoria en `docker-compose.yml` si es necesario.

### C√≥mo configurar remitentess de logs para diferentes componentes de CI/CD

Objetivo: Obtener logs de su pipeline en Logstash o Elasticsearch.

#### Opci√≥n 1: Usar Filebeat (remitente de logs liviano)

Instale [Filebeat][21] en sus hosts de CI/CD (GitHub runner, nodo de Jenkins, runner de GitLab, etc.).

Fragmento de configuraci√≥n de Filebeat (filebeat.yml):

```
# Configura Filebeat para recolectar logs de CI/CD y enviarlos a Logstash.
filebeat.inputs:
  - type: log  # Especifica entrada de archivo de log.
    enabled: true  # Habilita la entrada.
    paths:
      - /var/log/ci/*.log  # Recolecta logs del directorio especificado de CI.

output.logstash:
  hosts: ["localhost:5044"]  # Env√≠a logs a Logstash en el puerto 5044.
```

Luego ejecute:

```
# Ejecuta Filebeat con el archivo de configuraci√≥n especificado para la recolecci√≥n de logs.
filebeat -e -c filebeat.yml
```

#### Opci√≥n 2: Usar Vector.dev como una alternativa m√°s eficiente en recursos a Filebeat

Configuraci√≥n de Vector (vector.toml):

```
# Configura Vector para recolectar, analizar y enviar logs de CI/CD a Elasticsearch de manera eficiente.
[sources.ci_logs]
  type = "file"  # Especifica la recolecci√≥n de logs basada en archivos.
  include = ["/var/log/ci/*.log"]  # Apunta a archivos de logs de CI.

[transforms.json_parser]
  type = "remap"  # Usa remap transform para analizar logs.
  inputs = ["ci_logs"]  # Procesa logs desde la fuente ci_logs.
  source = '''
  . = parse_json!(.message)  # Analiza mensajes de log JSON a datos estructurados.
  '''

[sinks.to_elasticsearch]
  type = "elasticsearch"  # Env√≠a logs a Elasticsearch.
  inputs = ["json_parser"]  # Usa logs analizados del transformador json_parser.
  endpoint = "http://localhost:9200"  # Especifica el endpoint de Elasticsearch.
  index = "ci-logs"  # Almacena logs en el √≠ndice ci-logs.
```

Ejecute:

```
# Ejecuta Vector con el archivo de configuraci√≥n especificado para el procesamiento de logs.
vector -c vector.toml
```

### C√≥mo configurar patrones de √≠ndice y visualizaciones b√°sicas

Objetivo: Hacer que los logs de CI/CD sean consultables y visualizables en Kibana.

#### 1\. Abra Kibana ([http://localhost:5601][22])

-   Vaya a **Gesti√≥n de la pila ‚Üí Patrones de √≠ndice**
    
-   Cree un nuevo patr√≥n: `ci-logs-*`
    
-   Elija un campo de tiempo como `@timestamp`
    

#### 2\. Visualizaciones para casos de uso comunes de CI/CD

-   **Gr√°ficos de barras**: N√∫mero de construcciones fallidas vs completadas por d√≠a
    
-   **Gr√°fico de pastel**: Tipos de error principales o nombres de pruebas fallidas m√°s frecuentes
    
-   **Gr√°fico de l√≠neas**: Duraci√≥n de las construcciones a lo largo del tiempo (si se registra la duraci√≥n)
    

#### 3\. B√∫squedas guardadas y Tableros

Puede guardar una b√∫squeda como esta:

```
message: "error" AND job_name: "build"
```

Tambi√©n puede combinar visualizaciones en un Tablero de Salud de CI/CD.

## C√≥mo crear una estrategia de registro unificada a trav√©s de los componentes del pipeline

Crear una estrategia de registro unificada a trav√©s de los componentes de su pipeline de CI/CD asegura que los logs sean consistentes, rastreables y f√°ciles de correlacionar. Esto le ayuda a depurar problemas r√°pidamente, monitorear la salud del sistema y rastrear solicitudes a trav√©s de diferentes herramientas y servicios. Discutamos algunas pr√°cticas clave para lograr una estrategia de registro unificada:

### Implementaci√≥n de formatos de log consistentes a trav√©s de diferentes herramientas

Los formatos de log consistentes son importantes por varias razones. En primer lugar, un formato de log estandarizado permite consultar, buscar y visualizar m√°s f√°cilmente. Tambi√©n ayuda a la correlaci√≥n de logs de diferentes servicios. Y la consistencia tambi√©n asegura que todos los logs proporcionen detalles necesarios como la marca de tiempo, el nivel de log y el contexto de la solicitud.

**Formato JSON** es altamente recomendable ya que es estructurado, legible por m√°quinas y compatible con muchas herramientas de observabilidad (por ejemplo, Loki, Elasticsearch, Grafana).

Tambi√©n hay algunos campos clave que debes incluir:

-   `timestamp`: El momento en que se cre√≥ la entrada de registro (preferiblemente en UTC).
    
-   `log_level`: Indicar si el registro es un `INFO`, `ERROR`, `DEBUG`, etc.
    
-   `service`: El servicio o componente que genera el registro.
    
-   `message`: Una descripci√≥n concisa del evento o error.
    
-   `correlation_id`: Un identificador √∫nico para solicitudes para rastrear registros a trav√©s de los sistemas.
    

Aqu√≠ hay un ejemplo de un registro consistente en formato JSON:

```
{
  "timestamp": "2025-05-10T12:34:56Z",
  "log_level": "ERROR",
  "service": "ci_cd_pipeline",
  "message": "Error en la compilaci√≥n debido a una dependencia faltante",
  "correlation_id": "1234567890abcdef"
}
```

### C√≥mo Configurar el Reenv√≠o de Registros desde GitHub Actions, Jenkins o GitLab

El reenv√≠o de registros se refiere al env√≠o de registros desde tus tuber√≠as de CI/CD a un lugar central para f√°cil seguimiento. Es √∫til porque te permite detectar problemas r√°pidamente y depurar sin tener que buscar en archivos dispersos.

Para GitHub Actions, puedes configurar flujos de trabajo para escribir registros en un archivo o enviarlos directamente a una herramienta de agregaci√≥n de registros como Loki. En Jenkins, puedes usar scripts en la canalizaci√≥n para reenviar registros a un servidor de registros o sistema de archivos. De manera similar, para GitLab CI, puedes agregar scripts en `.gitlab-ci.yml` para reenviar registros a un punto final centralizado.

**Usando Acciones para Emitir Registros:**  
Puedes almacenar registros en archivos y luego reenviarlos a un sistema de registro (como Loki o Elasticsearch).  
Aqu√≠ tienes un ejemplo en un flujo de trabajo de GitHub Action:

```
# Define un flujo de trabajo de GitHub Actions para ejecutar pruebas y reenviar registros para observabilidad.
jobs:
  build:
    runs-on: ubuntu-latest  # Usa un ejecutor de Ubuntu.
    steps:
      - name: Checkout del repositorio  # Revisa el c√≥digo del repositorio.
        uses: actions/checkout@v2
      - name: Ejecutar pruebas y guardar salida de registro  # Ejecuta pruebas y guarda la salida en un archivo de registro.
        run: |
          echo "Iniciando pruebas..."
          npm test | tee test.log  # Captura la salida de las pruebas en test.log.
          # Reenv√≠a el archivo de registro a un punto final de Loki v√≠a HTTP POST.
          curl -X POST -F 'file=@test.log' http://tu-punto-final-de-loki
```

**Reenv√≠o de Registros con Promtail:**  
Si est√°s usando Grafana Loki para la agregaci√≥n de registros, configura Promtail para extraer los registros del ejecutor de GitHub Actions.

#### Jenkins:

Los registros de Jenkins pueden ser reenviados a sistemas externos (como Elasticsearch o Loki) utilizando transportadores de registros o complementos.

**Puedes usar el Plugin de Logstash** para reenviar registros de Jenkins a una pila ELK u otros sistemas:

-   Instala el plugin de Logstash en Jenkins.
    
-   Configura el plugin para reenviar registros a un servidor de Elasticsearch o a un sistema de registro de tu elecci√≥n.
    
-   En Jenkins, agrega configuraciones de reenv√≠o de registros:
    

```
pipeline {
  agent any
  stages {
    stage('Build') {
      steps {
        script {
          // Ejemplo de reenv√≠o de registros a un servidor de registros
          sh 'echo "Compilaci√≥n exitosa" | curl -X POST -d @- http://tu-servidor-de-registros'
        }
      }
    }
  }
}
```

**Reenviar a Loki:**  
Jenkins admite el controlador de registros `loki` para contenedores si ejecutas Jenkins en Docker. Puedes enviar registros directamente a Loki usando este controlador:

```
# Ejecuta un contenedor de Jenkins con el controlador de registros de Loki para enviar registros directamente a Loki.
docker run --log-driver=loki --log-opt loki-url=http://loki:3100 jenkins/jenkins:lts
```

#### GitLab:

GitLab CI permite que los registros sean reenviados a sistemas externos para recopilaci√≥n y an√°lisis centralizados.

**Usar GitLab CI/CD para Emitir Registros**:  
Ejemplo en `.gitlab-ci.yml`:

```
# Define una tuber√≠a GitLab CI/CD para ejecutar una compilaci√≥n y reenviar registros a Loki.
stages:
  - build
build:
  script:
    - echo "Iniciando la compilaci√≥n" | tee build.log  # Guarda la salida de la compilaci√≥n en build.log.
    - curl -X POST -d @build.log http://tu-punto-final-de-loki  # Reenv√≠a el registro a Loki.
```

**Ejecutores de GitLab**:  
Configura los ejecutores de GitLab para reenviar registros a un servicio externo como Loki o Elasticsearch usando configuraciones `log-driver` o el transportador de registros `fluentd`.

### C√≥mo Agregar IDs de Correlaci√≥n para Rastrear Solicitudes a Trav√©s del Sistema

#### Por qu√© los IDs de Correlaci√≥n Son Importantes:

Los IDs de correlaci√≥n te permiten rastrear una sola solicitud a medida que viaja a trav√©s de diferentes servicios y herramientas, proporcionando visibilidad y soluci√≥n de problemas de extremo a extremo.

Son cr√≠ticos para depurar sistemas distribuidos, especialmente cuando diferentes servicios (por ejemplo, herramienta CI, herramienta de despliegue, servicio API) est√°n involucrados.

#### C√≥mo Agregar IDs de Correlaci√≥n:

Puedes usar un UUID (Identificador √önico Universal) o un GUID (Identificador √önico Global) para generar un ID √∫nico para cada solicitud.

Si est√°s utilizando microservicios o m√∫ltiples servicios en la tuber√≠a, solo aseg√∫rate de que el mismo ID se propague a trav√©s de cada servicio.

Muchas bibliotecas de registro (por ejemplo, `winston` para Node.js, `log4j` para Java) admiten la generaci√≥n autom√°tica de IDs de correlaci√≥n y el registro.

Aqu√≠ tienes un ejemplo en Node.js (usando `winston`):

```
// Configura Winston para registro estructurado con IDs de correlaci√≥n en una tuber√≠a CI/CD.
const { createLogger, transports, format } = require('winston');
const { printf } = format;

```

```markdown
// Genera un ID de correlaci√≥n aleatorio para rastrear solicitudes.
function generateCorrelationId() {
  return Math.random().toString(36).substring(2, 15);
}

// Registra un mensaje de muestra.
logger.info('La ejecuci√≥n del pipeline ha comenzado');
```

#### C√≥mo Propagar IDs de Correlaci√≥n Entre Servicios:

En herramientas CI/CD, puedes configurar tu pipeline para inyectar el ID de correlaci√≥n en los registros. Por ejemplo, en GitHub Actions, puedes generar un ID de correlaci√≥n en la secci√≥n `env` y propagarlo en cada trabajo:

```
# Define un flujo de trabajo de GitHub Actions que incluye un ID de correlaci√≥n para el rastreo de registros.
jobs:
  build:
    runs-on: ubuntu-latest  # Usa un runner de Ubuntu.
    env:
      CORRELATION_ID: ${{ github.run_id }}  # Usa el ID de ejecuci√≥n de GitHub como el ID de correlaci√≥n.
    steps:
      - name: Checkout repository  # Revisa el c√≥digo del repositorio.
        uses: actions/checkout@v2
      - name: Log build start with correlation ID  # Registro el inicio de la build con el ID de correlaci√≥n.
        run: echo "Build started with Correlation ID: $CORRELATION_ID"
```

#### Incluye IDs de Correlaci√≥n en Todos los Registros:

Querr√°s asegurarte de que los registros de todos los componentes en el pipeline (GitHub Actions, Jenkins, GitLab, herramientas de despliegue, etc.) incluyan el ID de correlaci√≥n como parte del mensaje del registro. Esto te permite rastrear los registros de una √∫nica solicitud o ejecuci√≥n de pipeline a trav√©s de diferentes servicios.

#### Visualiza Tu Flujo de Registros

Puedes crear un diagrama que muestre c√≥mo los registros se mueven desde tu herramienta CI/CD (por ejemplo, GitHub Actions) hacia Promtail/Vector, luego a Loki/Elasticsearch y finalmente a Grafana/Kibana para su visualizaci√≥n. Utiliza herramientas como [Draw.io][23] para mapear el flujo de observabilidad de tu pipeline.

## C√≥mo Consultar y Analizar Registros para una Resoluci√≥n de Problemas Efectiva

En esta secci√≥n, aprender√°s c√≥mo usar LogQL (el lenguaje de consultas de Loki) para filtrar el ruido y encontrar los registros espec√≠ficos que importan. Ya sea que est√©s buscando una falla misteriosa en la compilaci√≥n o rastreando problemas de despliegue a trav√©s de m√∫ltiples servicios, estos patrones de consulta siempre ayudan.

![Gr√°fico de barras que muestra los resultados de las builds CI/CD del 20 al 26 de mayo de 2025. Las barras azules representan builds exitosas que van de 39 a 52 por d√≠a, mientras que las barras rojas muestran builds fallidas que van de 1 a 9 por d√≠a. El gr√°fico demuestra consistentemente altas tasas de √©xito con bajas tasas de fallas a lo largo de la semana, con el 23 de mayo mostrando el mayor conteo de fallas con 9 builds.](https://cdn.hashnode.com/res/hashnode/image/upload/v1748224707087/d348accc-0ef8-4ebb-9cb9-49995404b0ec.png)

Este gr√°fico de barras ilustra el rendimiento de las builds CI/CD del 20 al 26 de mayo de 2025. Compara el n√∫mero de builds exitosas (en azul) con las fallidas (en rosa) cada d√≠a. Las builds exitosas consistentemente se sit√∫an entre 40 y 50, mientras que las fallidas alcanzan su pico de 10 el 23 de mayo, con otros d√≠as mostrando entre 2 y 8 fallas. Esto indica un pipeline generalmente estable con problemas ocasionales.

### C√≥mo Escribir Consultas Avanzadas de LogQL para Identificar Problemas en CI/CD

LogQL es el lenguaje de consultas de Grafana Loki, dise√±ado para consultar registros con una sintaxis similar al PromQL de Prometheus. Permite b√∫squedas de registros eficientes y es particularmente √∫til para resolver problemas en CI/CD.

#### Sintaxis B√°sica de LogQL:

**1\. Flujos de Registro:**

```
{job="ci_cd", level="error"}
```

Esta consulta recupera registros donde la etiqueta `job` es `ci_cd` y la etiqueta `level` es `error`.

**2\. Filtros de Registro:**

```
{job="ci_cd"} |= "build failed"
```

El operador `|=` filtra los registros para incluir solo aquellos que contienen la cadena especificada, por ejemplo "build failed".

**3\. Expresiones Regulares:**

```
{job="ci_cd"} |~ "error.*timeout"
```

Esto usa el operador `|~` para filtrar registros usando una expresi√≥n regular. En este caso, encuentra registros que contienen un "error" seguido de "timeout".

#### Consultas Avanzadas de LogQL para Problemas en CI/CD:

**1\. Filtrar Registros para Falla de Builds Espec√≠ficas:**

Si tu pipeline utiliza una etiqueta espec√≠fica para los nombres de las builds:

```
{job="ci_cd", build="build123"} |= "failure"
```

Esto encuentra registros relacionados con el trabajo `build123` que contienen la palabra "failure".

**2\. Usando Rango de Tiempo y Agrupaci√≥n:**

Para encontrar registros de errores en los √∫ltimos 15 minutos:

```
{job="ci_cd", level="error"} | "build failed" | range(start="15m")
```

Para agrupar registros por trabajo y tipo de error:

```
sum by (job) (count_over_time({job="ci_cd", level="error"}[5m]))
```

Esto devolver√° el conteo de registros de error por trabajo, agrupados por el nombre del trabajo, en los √∫ltimos 5 minutos.

### C√≥mo Crear Consultas Espec√≠ficas para el Pipeline para Patrones Comunes de Fallas

#### Patrones Comunes de Fallas en Pipelines CI/CD:

**1\. Fallas de Builds:**

Si los registros de tu sistema CI contienen errores de compilaci√≥n, puedes identificarlos con:

```
{job="ci_cd", level="error"} |= "build failed"
```

Puedes extender esto para filtrar por pasos o etapas espec√≠ficas, por ejemplo, ‚Äútest failed‚Äù, o ‚Äúcompilation error‚Äù.

**2\. Fallas de Pruebas:**

Los registros de tu corredor de pruebas (por ejemplo, Jest, Mocha, JUnit) pueden contener mensajes de falla espec√≠ficos:

```
{job="ci_cd", stage="test"} |= "test failed"
```

**3\. Problemas de Dependencias:**

Si tu pipeline falla debido a dependencias faltantes o en conflicto, busca errores relacionados con `npm`, `maven` o `docker`:

```
{job="ci_cd", image="node"} |= "npm ERR!"
```
```

```
{job="ci_cd", image="maven"} |= "[ERROR]"
```

**4\. Restricciones de Recursos (por ejemplo, Falta de Memoria):**

Si experimenta restricciones de recursos, podr√≠a ver registros como "OutOfMemoryError":

```
{job="ci_cd", level="error"} |= "OutOfMemoryError"
```

**Ejemplo de combinaci√≥n de filtros:**

```
{job="ci_cd", level="error"} |= "build failed" |~ "timeout|dependency" | range(start="1h")
```

Esto combina filtros de registros para "build failed", coincidiendo con cualquier registro que contenga los t√©rminos "timeout" o "dependency", de la √∫ltima hora.

### C√≥mo Configurar Reglas de Alertas Basadas en Patrones de Logs

Las alertas ayudan a detectar problemas recurrentes de manera proactiva. Le notifican cuando aparece un patr√≥n espec√≠fico en sus registros, permiti√©ndole tomar acci√≥n r√°pidamente.

#### **Pasos para Configurar Alertas:**

**1\. Cree una Consulta para la Alerta:**

Primero, defina el patr√≥n de log que desea monitorear. Por ejemplo, una alerta para fallos en la construcci√≥n:

```
{job="ci_cd", level="error"} |= "build failed"
```

**2\. Cree una Alerta en Grafana:**

Siga estos pasos para configurar alertas en Grafana:

-   Vaya a su panel de control de Grafana.
    
-   Elija el panel en el que desea configurar la alerta (o cree un nuevo panel para este prop√≥sito).
    
-   En el panel, haga clic en la pesta√±a **Alerta**.
    
-   Establezca el campo **Consulta** en su consulta LogQL, como la anterior.
    
-   Bajo **Condiciones**, defina cu√°ndo debe activarse la alerta, por ejemplo, si el error ocurre m√°s de `3` veces en `5 minutos`.
    

**3\. Configuraci√≥n de la Alerta:**

Ahora querr√° configurar el intervalo de evaluaci√≥n de la alerta y las condiciones para activar la alerta (por ejemplo, si la consulta devuelve resultados por encima de un cierto umbral).

**Aqu√≠ hay un ejemplo:** Activa una alerta si el n√∫mero de errores excede 5 en 5 minutos:

```
count_over_time({job="ci_cd", level="error"} |= "build failed"[5m]) > 5
```

**4\. Establecer Notificaciones de Alertas:**

Puede elegir d√≥nde quiere que se env√≠e la alerta (como a Slack, correo electr√≥nico o PagerDuty). Y Grafana se puede integrar con estos sistemas para enviar alertas en tiempo real a los miembros correctos del equipo.

**Consulta de alerta de ejemplo para fallos de prueba:**

```
count_over_time({job="ci_cd", stage="test"} |= "test failed"[5m]) > 3
```

Esta consulta activa una alerta si se registran m√°s de 3 fallos de prueba en los √∫ltimos 5 minutos.

### Profundizaci√≥n en el Lenguaje de Consulta de Kibana para Contextos CI/CD

El Lenguaje de Consulta de Kibana (KQL) es una herramienta poderosa para buscar y filtrar registros dentro de Elasticsearch, y se vuelve especialmente √∫til para depurar pipelines de CI/CD.

#### Sintaxis B√°sica de Consulta:

-   **Campo:**
    
    ```
      textCopyEditfieldname:value
    ```
    
    Ejemplo: `status: "failure"`
    
-   **Comod√≠n:** Use `*` para coincidir con cualquier n√∫mero de caracteres:
    
    ```
      textCopyEditmessage: "test*"
    ```
    
-   **Consultas de Rango:** Para buscar registros dentro de un marco de tiempo espec√≠fico:
    
    ```
      textCopyEdittimestamp:[2023-05-01 TO 2023-05-15]
    ```
    
-   **Consultas Booleanas:** Combine consultas usando `AND`, `OR`, y `NOT`:
    
    ```
      textCopyEditstatus: "failure" AND build_id: "12345"
    ```
    

#### Consultas Basadas en el Tiempo:

Dado que los registros de CI/CD suelen estar ligados a operaciones sensibles al tiempo (construcciones, despliegues), KQL le permite filtrar registros por tiempo:

```
textCopyEdit@timestamp:[now-1d TO now]
```

#### Consultas Anidadas (Para Pipelines Complejas):

Los registros de CI/CD pueden tener estructuras anidadas o multinivel (por ejemplo, registros dentro de contenedores). Puede consultar estos campos anidados:

```
textCopyEditpipeline.logs.message: "build failed"
```

#### Agregaciones y Agrupaciones:

Puede agregar registros bas√°ndose en ciertos campos para identificar tendencias o problemas recurrentes:

```
textCopyEditterms aggregation on "status" field
```

Esto ayuda a identificar los estados de fallo m√°s comunes en su pipeline.

#### Filtrado Espec√≠fico de Campo:

Al depurar componentes espec√≠ficos como una herramienta de construcci√≥n o paso de despliegue, puede filtrar por esos campos espec√≠ficos del componente:

```
textCopyEditbuild_tool: "Jenkins" AND status: "failure"
```

#### Creaci√≥n de B√∫squedas Guardadas para Problemas Recurrentes

Una vez que haya creado consultas que le ayuden a identificar problemas comunes en su pipeline de CI/CD, puede guardarlas en Kibana para un uso futuro.

**1\. Crear una B√∫squeda Guardada:**

Ejecute su consulta deseada en la pesta√±a Discover de Kibana. Haga clic en el bot√≥n ‚ÄúGuardar‚Äù y dele un nombre significativo, como "Failed Builds - Last Week". Puede agregar filtros y personalizar el rango de tiempo para que coincidan con sus patrones t√≠picos de problemas.

**2\. Usar Filtros para Identificar Problemas Recurrentes:**

Cree b√∫squedas guardadas que se enfoquen en problemas recurrentes espec√≠ficos como:

-   Fallos de construcci√≥n basados en una herramienta o versi√≥n espec√≠fica.
    
-   Fallos de prueba dentro de un m√≥dulo o conjunto de pruebas particular.
    

B√∫squeda de ejemplo para ‚Äúpruebas inestables‚Äù:

```
textCopyEdittest_status: "failed" AND error_message: "*timeout*"
```

**3\. Guardar M√∫ltiples Variaciones:**

Puede guardar m√∫ltiples variaciones de consultas basadas en diferentes tipos de errores o herramientas de CI/CD:

-   **Trabajos Fallidos:** `status: "failure"`
    
-   **Fallos de Prueba en Construcci√≥n:** `log_type: "test" AND status: "failure"`
    
-   **Restricciones de Recursos:** `error_message: "*memory*"`
    

Estas b√∫squedas guardadas le permitir√°n solucionar r√°pidamente problemas espec√≠ficos que ocurren con frecuencia.
```

Una vez que hayas guardado b√∫squedas, Kibana te permite crear visualizaciones a partir de tus datos, facilitando la identificaci√≥n de tendencias, anomal√≠as o patrones a lo largo del tiempo.

**1\. Crear una Visualizaci√≥n:**

Ve a la pesta√±a **Visualizar** en Kibana. Selecciona el tipo de visualizaci√≥n apropiado. Las visualizaciones comunes para depurar pipelines CI/CD incluyen:

-   **Gr√°fico de L√≠neas:** Rastrea las tasas de fallos de compilaci√≥n a lo largo del tiempo.
    
-   **Gr√°fico de Barras:** Muestra el n√∫mero de fallos por herramienta o servicio CI.
    
-   **Gr√°fico de Tartas:** Desglose de las razones de fallo (por ejemplo, errores de compilaci√≥n, fallos de pruebas, restricciones de recursos).
    

**2\. Rastrear Tendencias de Fallos a lo Largo del Tiempo:**

Crea un gr√°fico de l√≠neas para rastrear los fallos de compilaci√≥n durante un periodo dado:

-   **Eje X:** Tiempo (por ejemplo, diario o semanal).
    
-   **Eje Y:** Conteo de fallos de compilaci√≥n.
    
-   **Agregaci√≥n:** Histograma de fechas con el campo `@timestamp`.
    

Esto te ayudar√° a visualizar c√≥mo est√°n marcando tendencia los fallos de compilaci√≥n, facilitando la identificaci√≥n de problemas recurrentes o picos en los fallos.

**3\. Monitorear Tipos de Fallos por Herramienta CI:**

Crea un gr√°fico de barras que muestre el n√∫mero de fallos desglosado por herramienta CI:

-   **Eje X:** Herramienta CI (Jenkins, GitHub Actions, GitLab, etc.).
    
-   **Eje Y:** Conteo de fallos.
    
-   **Agregaci√≥n:** Agregaci√≥n de t√©rminos en el campo `ci_tool`.
    

Esta visualizaci√≥n ayuda a identificar qu√© herramienta CI est√° experimentando la mayor cantidad de fallos y enfocar los esfuerzos de soluci√≥n de problemas all√≠.

**4\. Visualizar Mensajes de Error por Frecuencia:**

Puedes visualizar qu√© mensajes de error aparecen con m√°s frecuencia, ayud√°ndote a entender qu√© podr√≠a estar causando problemas recurrentes:

-   **Eje X:** Tipo de mensaje de error.
    
-   **Eje Y:** Conteo de ocurrencias.
    
-   **Agregaci√≥n:** Agregaci√≥n de t√©rminos en el campo `error_message`.
    

**5\. Panel para Monitoreo Hol√≠stico:**

Crea un panel que re√∫na m√∫ltiples visualizaciones. Puedes tener un gr√°fico para las tendencias de fallos, otro para tipos de fallos (gr√°fico de barras), y un gr√°fico de tartas mostrando el porcentaje de fallos causados por diferentes problemas. Este panel te da una vista hol√≠stica de la salud de tu pipeline.

#### T√©cnicas Avanzadas de Visualizaci√≥n:

Hay varias t√©cnicas avanzadas que puedes usar para profundizar m√°s en tus datos.

-   **Mapas de Calor:** Usa mapas de calor para detectar anomal√≠as basadas en el tiempo en las duraciones de las compilaciones o fallos de pruebas.
    
-   **Detecci√≥n de Anomal√≠as:** Kibana tiene detecci√≥n de anomal√≠as incorporada que se puede aplicar a datos de logs para detectar autom√°ticamente patrones que se desv√≠an de la norma. Esto es especialmente √∫til para detectar errores raros o inesperados en tu pipeline CI/CD.
    
    Ejemplo para detecci√≥n de anomal√≠as:
    
    ```
      textCopyEditfield: duraci√≥n
      agregaci√≥n: promedio
      modelo de detecci√≥n de anomal√≠as: "l√≠nea base"
    ```
    

## C√≥mo Configurar M√©tricas de Prometheus junto a tus Logs

Para entender completamente la salud y el rendimiento de tu pipeline CI/CD, combinar m√©tricas y logs es esencial. Prometheus es una excelente herramienta para capturar m√©tricas de series temporales, y funciona perfectamente con Grafana y Loki (o cualquier sistema de agregaci√≥n de logs).

### **C√≥mo Configurar Prometheus para la Colecci√≥n de M√©tricas CI/CD:**

#### 1\. Instalar Prometheus:

Puedes instalar Prometheus usando Docker o Kubernetes para una implementaci√≥n sencilla.

Para instalaci√≥n basada en Docker:

```
docker run -d -p 9090:9090 --name prometheus prom/prometheus
```

#### **2\. Configurar Prometheus para Recopilar M√©tricas:**

Prometheus necesita ser configurado para recopilar m√©tricas de tus servicios CI/CD.

Edita el archivo `prometheus.yml`:

```
scrape_configs:
  - job_name: 'ci_cd_metrics'
    static_configs:
      - targets: ['localhost:8080', 'localhost:9091']
```

#### 3\. Instrumentar tus Servicios CI/CD:

Para exponer m√©tricas, necesitas integrar librer√≠as cliente de Prometheus en tus servicios CI/CD.

Por ejemplo, para exponer m√©tricas de compilaci√≥n desde un trabajo de Jenkins, usa el [plugin de Prometheus para Jenkins][24]. En GitHub Actions, puedes usar [Prometheus][25] para exponer m√©tricas de trabajo.

#### **4\. Exponer el Endpoint de M√©tricas:**

Querr√°s asegurarte de que tus servicios exponen un endpoint `/metrics` que Prometheus pueda recolectar. Por ejemplo, usa librer√≠as cliente de Prometheus en tu aplicaci√≥n para exponer este endpoint.

#### Soluci√≥n de Problemas en la Configuraci√≥n de Prometheus

Si Prometheus falla al iniciar o al recopilar m√©tricas, aqu√≠ hay algunas cosas que podr√≠an estar fallando:

1.  **Ca√≠das del Contenedor:** Revisa los logs con `docker logs prometheus`. Busca errores como ‚Äúpuerto ya en uso‚Äù (por ejemplo, el 9090) o problemas de an√°lisis de configuraci√≥n.
    
    -   Soluci√≥n: Cambia el puerto en `docker run` (por ejemplo, `-p 9091:9090`) o corrige la sintaxis del archivo `prometheus.yml`.
2.  **M√©tricas No Recopiladas:** Verifica que los objetivos sean alcanzables usando `docker logs prometheus` o prueba con curl `http://localhost:9090/targets`. Revisa `prometheus.yml` para asegurar que los endpoints sean correctos.
    
    -   Soluci√≥n: Actualiza `targets` en `scrape_configs` (por ejemplo, `localhost:8080`) para que coincidan con el endpoint de m√©tricas de tu servicio CI/CD.
3.  **Restricciones de Recursos:** Monitorea el uso con `docker stats` o `top` en el host.
    
    -   Soluci√≥n: Aseg√∫rate de tener al menos 4GB de RAM y 10GB de espacio en disco. Incrementa la retenci√≥n de almacenamiento o reduce la frecuencia de recopilaci√≥n en `prometheus.yml` si es necesario.

Una vez que Prometheus est√© recopilando m√©tricas, el siguiente paso es visualizarlas y correlacionarlas en Grafana.

### **C√≥mo Integrar Prometheus con Grafana:**

Primero, necesitar√°s instalar Grafana. Puedes utilizar Docker o Kubernetes para una implementaci√≥n r√°pida:

```
docker run -d -p 3000:3000 --name grafana grafana/grafana
```

A continuaci√≥n, configura Grafana para usar Prometheus como fuente de datos. Para hacer esto, inicia sesi√≥n en Grafana (`localhost:3000` por defecto). Ve a `Configuration` > `Data Sources` > `Add Data Source` > Elige `Prometheus`. Ingresa la URL de tu servidor Prometheus (por ejemplo, `http://localhost:9090`) y haz clic en `Save & Test`.

Ahora es momento de construir un panel unificado. Para hacerlo, crea un nuevo panel en Grafana que combine tanto logs (Loki) como m√©tricas (Prometheus).

A√±ade un panel con consultas de datos de Prometheus para visualizar m√©tricas de la canalizaci√≥n como la tasa de √©xito de construcci√≥n, la duraci√≥n de la implementaci√≥n y la cantidad de fallos. Usa el tipo de visualizaci√≥n `Graph` para datos de series temporales y `Stat` para m√©tricas de resumen r√°pido.

Finalmente, en el mismo panel de Grafana, agrega paneles para logs (de Loki u otro sistema de registro). Utiliza el panel `Logs` para visualizar datos de registro y enlazarlos con las m√©tricas relevantes de Prometheus mediante correlaciones basadas en el tiempo.

**Ejemplo**: Si se detecta un pico en el uso de CPU (m√©trica de Prometheus), el panel de logs podr√≠a mostrar logs relacionados, como errores o trabajos de construcci√≥n fallidos.

## C√≥mo Usar Ejemplares para Saltar de M√©tricas a Logs Relevantes

Los ejemplares son una caracter√≠stica avanzada en Prometheus que te permite conectar datos de m√©tricas con logs y trazas. Grafana admite esta caracter√≠stica, y puede ser incre√≠blemente √∫til cuando se investigan problemas.

### C√≥mo Configurar Ejemplares en Prometheus:

**1\. Habilitar Ejemplares en Tu Aplicaci√≥n:**

Los ejemplares son esencialmente trazas incrustadas en tus m√©tricas. Para usarlos, necesitas asegurarte de que tu aplicaci√≥n est√© instrumentada para enviar datos de ejemplares junto con tus m√©tricas.

Muchas bibliotecas admiten la adici√≥n de ejemplares a m√©tricas de Prometheus, como `prom-client` (Node.js) y `prometheus-net` (C#).

Aqu√≠ hay un ejemplo en Node.js:

```
// Demuestra c√≥mo agregar un ejemplar a una m√©trica de Prometheus para enlazar con logs o trazas.
const promClient = require('prom-client');

// Crea una m√©trica de contador para rastrear construcciones fallidas de CI/CD.
const counter = new promClient.Counter({
  name: 'ci_cd_failed_builds_total',  // Nombre de la m√©trica para construcciones fallidas.
  help: 'Total de construcciones fallidas',  // Descripci√≥n de la m√©trica.
});

// Incrementa el contador con un ejemplar para trazar.
counter.inc({ exemplar: 'build_failed' });
```

**2\. Habilitar Ejemplares en la Configuraci√≥n de Prometheus:**

Aseg√∫rate de que tu servidor Prometheus est√© configurado para almacenar y exponer ejemplares. Los ejemplares suelen incluirse con m√©tricas de histograma o resumen, as√≠ que aseg√∫rate de haberlos configurado correctamente.

**3\. Visualizar Ejemplares en Grafana:**

En Grafana, cuando consultas a Prometheus por m√©tricas con ejemplares, Grafana mostrar√° los logs o trazas vinculadas cuando pases el cursor sobre una m√©trica.

Utiliza la opci√≥n `Exemplar` en los paneles de Grafana para acceder r√°pidamente a los logs de m√©tricas espec√≠ficas.

Por ejemplo, si tienes una m√©trica `build_failure_total` y detectas una falla en tu canalizaci√≥n, puedes hacer clic en la m√©trica de falla en Grafana y ver instant√°neamente los logs relevantes para esa falla espec√≠fica usando los ejemplares.

## C√≥mo Diagnosticar y Solucionar Problemas Comunes de CI/CD

Las canalizaciones de CI/CD a menudo encuentran problemas como fallos de construcci√≥n, problemas de dependencias y pruebas inestables que pueden interrumpir los flujos de trabajo de desarrollo. Esta secci√≥n proporciona estrategias pr√°cticas para diagnosticar y resolver estos problemas comunes mediante el an√°lisis de logs y t√©cnicas de depuraci√≥n sistem√°tica, ayud√°ndote a restaurar la estabilidad de la canalizaci√≥n r√°pidamente.

### **Estrategia 1: Depuraci√≥n Sistem√°tica de Fallos de Construcci√≥n**

Los fallos de construcci√≥n son un desaf√≠o frecuente de CI/CD, a menudo provenientes de errores en el c√≥digo, pruebas o configuraciones. La depuraci√≥n sistem√°tica de estos problemas implica analizar logs para identificar las causas ra√≠z, utilizando los siguientes enfoques.

#### Identificaci√≥n de Patrones en la Salida del Compilador y de Pruebas

Cuando depuras fallos de construcci√≥n, primero necesitas examinar los logs del compilador y la salida de las pruebas. Veamos algunas estrategias clave.

#### 1\. Buscar Mensajes de Error Espec√≠ficos:

Hay algunos tipos comunes de mensajes de error que podr√≠as recibir. Son:

-   **Errores de sintaxis**: Busca l√≠neas que indiquen que hay un desajuste en la sintaxis, como puntos y comas faltantes, variables no declaradas o llamadas a funciones incorrectas.
    
-   **Errores de enlazador**: Estos suelen ocurrir cuando no se encuentran las bibliotecas o dependencias requeridas. Normalmente ver√°s errores como `referencia indefinida` o `s√≠mbolo no encontrado`.
    
-   **Errores de herramientas de construcci√≥n**: Si est√°s utilizando sistemas de construcci√≥n como Maven, Gradle o MSBuild, sus logs proporcionar√°n c√≥digos de error espec√≠ficos o configuraciones faltantes.
    

#### 2\. Buscar Patrones Comunes de Error:

A menudo, las construcciones fallidas repiten el mismo error o patr√≥n en m√∫ltiples ejecuciones. Revisa los logs para t√©rminos recurrentes o errores que apunten a m√≥dulos o funciones espec√≠ficos. Y recuerda que agrupar problemas similares puede ayudarte a identificar la causa ra√≠z m√°s r√°pido.

#### 3\. Usar Expresiones Regulares para Filtrado de Logs:

**Como ejemplo:**

-   Si la compilaci√≥n falla con un error de "Falta de Memoria", busque problemas de asignaci√≥n de memoria o configuraciones que puedan aumentarse.
    
-   Si los fallos de prueba est√°n relacionados con m√≥dulos espec√≠ficos, inspeccione esos m√≥dulos para verificar cambios recientes o problemas de dependencia.
    

### Estrategia 2: Soluci√≥n de Problemas de Dependencias con An√°lisis de Logs

Los problemas de dependencias son comunes en las fallas de compilaci√≥n, especialmente en pipelines CI/CD complejos con m√∫ltiples m√≥dulos o servicios. Para resolver estos problemas, considere lo siguiente:

**1\. Verifique Dependencias Faltantes o Desactualizadas**:

Comience revisando la salida de la herramienta de compilaci√≥n para verificar mensajes relacionados con dependencias faltantes (por ejemplo, `dependencia no encontrada`, `conflicto de versi√≥n`).

Muchas herramientas de compilaci√≥n (como Maven, npm o .NET) incluir√°n mensajes de error espec√≠ficos cuando una dependencia falta o es incompatible.

**2\. Inspeccione los Registros de Resoluci√≥n de Dependencias**:

Algunas herramientas de compilaci√≥n proporcionan registros detallados que muestran c√≥mo se resolvieron las dependencias (por ejemplo, la versi√≥n de una biblioteca que se utiliz√≥). Estos registros pueden mostrarle si hay un desajuste de versi√≥n.

Aseg√∫rese de que sus archivos `package.json` (para proyectos de JavaScript), `pom.xml` (para Java) o `csproj` (para C#) est√©n definidos correctamente con versiones compatibles.

**3\. Verifique Conectividad de Red**:

Las herramientas CI/CD a veces fallan al obtener dependencias debido a problemas de red (por ejemplo, configuraciones de proxy, acceso al repositorio). Busque cualquier error que indique que un repositorio no pudo ser alcanzado.

**4\. Ejemplo de Registro:**

Si un proyecto Java falla con `No se pudo encontrar el artefacto`, es probable que falte una dependencia o que no sea accesible. Verifique la URL del repositorio o si el artefacto existe en su repositorio Maven.

**5\. Resolver Conflictos de Versi√≥n**:

Los conflictos de versi√≥n ocurren cuando diferentes dependencias requieren versiones incompatibles de la misma biblioteca. Esto es especialmente cierto en proyectos Java (con Maven/Gradle) y .NET. Considere usar herramientas para resolver conflictos de versi√≥n autom√°ticamente o definir versiones compatibles manualmente.

### Soluci√≥n de Pruebas Inestables Basadas en Datos Hist√≥ricos de Registros

**Nota:** Problemas como fallos de contenedores, registros no ingeridos o limitaciones de recursos aqu√≠ pueden parecerse a los de otras secciones. Estos son comunes en servicios y procesos CI/CD, pero cada secci√≥n ofrece un contexto √∫nico para evitar redundancias.

Las pruebas inestables ‚Äì es decir, aquellas que a veces pasan y otras fallan ‚Äì son comunes en pipelines CI/CD, y pueden ser frustrantes. Discutamos algunas estrategias para abordarlas:

**1\. Analizar Registros de Pruebas a lo Largo del Tiempo**:

Revise registros hist√≥ricos para identificar patrones de cuando la prueba falla. Busque problemas de tiempo, l√≠mites de recursos o dependencias externas que puedan afectar la fiabilidad de las pruebas.

Por ejemplo, si una prueba falla intermitentemente despu√©s de cierto tiempo o solo durante etapas espec√≠ficas del pipeline, podr√≠a indicar agotamiento de recursos o condiciones de carrera.

**2\. Verificar Dependencias de Pruebas**:

A menudo, las pruebas inestables dependen de servicios o recursos externos (por ejemplo, bases de datos, APIs, sistemas de archivos). Verifique si estos servicios est√°n consistentemente disponibles y correctamente simulados durante la ejecuci√≥n de pruebas.

Registros que mencionan conexiones fallidas a servicios externos o ambientes inestables pueden darle pistas sobre posibles problemas con las dependencias.

**3\. Ejecutar Pruebas con Aumento de Registros**:

Aumente la verbosidad de los registros de pruebas para capturar m√°s informaci√≥n sobre los fallos. Esto puede ayudarle a detectar por qu√© las pruebas fallan en ciertas condiciones.

Por ejemplo, agregar registros de depuraci√≥n dentro de las pruebas puede proporcionar m√°s contexto sobre el estado de la aplicaci√≥n cuando ocurre el fallo.

**4\. Problemas Relacionados con la Hora del D√≠a**:

Algunas pruebas inestables pueden fallar durante horas de uso m√°ximo, especialmente si dependen de recursos compartidos. Busque patrones que correlacionen con la contenci√≥n de recursos (por ejemplo, bloqueos de bases de datos, l√≠mites de tasa de API).

Registros que muestran alto uso de CPU o memoria pueden indicar que las restricciones de recursos est√°n afectando la estabilidad de sus pruebas.

**5\. Implementar L√≥gica de Reintento para Pruebas Inestables**:

Para mitigar los efectos de las pruebas inestables, implemente reintentos autom√°ticos para pruebas que fallen de manera intermitente. Esto puede ayudar a reducir el ruido en su pipeline CI/CD mientras usted investiga las causas ra√≠ces.

Por ejemplo, si una prueba de conexi√≥n a base de datos falla de manera intermitente, puede querer inspeccionar los registros de la base de datos en busca de se√±ales de timeouts o agotamiento del pool de conexiones.

### C√≥mo Resolver Fallas en Pipeline de Despliegue

Las fallas en el pipeline de despliegue pueden deberse a varias fuentes, y diagnosticarlas requiere un enfoque sistem√°tico usando registros y herramientas de observabilidad disponibles. A continuaci√≥n, describiremos los patrones comunes en los registros que indican limitaciones de recursos, problemas de permisos/autenticaci√≥n y desviaci√≥n de configuraci√≥n entre ambientes.

**Patrones de Registro que Indican Limitaciones de Recursos**

Las limitaciones de recursos son una causa com√∫n de fallas en el pipeline. Estas pueden incluir l√≠mites de CPU, uso de memoria o espacio de disco agotado. As√≠ es como se reconocen estos patrones:

#### Indicadores Clave en los Registros:

-   **Problemas de Memoria**: Busque mensajes como _"falta de memoria"_, _"l√≠mite de memoria excedido"_ o _"OOM killed"_ en sus registros. Aqu√≠ hay un ejemplo en registros de Kubernetes:

-   **L√≠mites de CPU**: Presta atenci√≥n a los registros que muestren que un proceso super√≥ los l√≠mites de CPU o fue limitado. Aqu√≠ tienes un ejemplo:

```
el proceso 'foo' alcanz√≥ el l√≠mite de CPU, reduciendo a 100%
```

-   **Espacio en Disco**: Los registros pueden mostrar errores al escribir archivos o mensajes acerca de que un disco est√° lleno. Aqu√≠ tienes un ejemplo:

```
No se puede escribir en el archivo, el espacio en el disco est√° lleno.
```

Puedes resolver los problemas de memoria aumentando la memoria asignada a tus contenedores, m√°quina virtual o instancias en la nube.

Puedes resolver los problemas de CPU ajustando los l√≠mites de CPU o escalando tu infraestructura para agregar m√°s recursos.

Y finalmente, puedes resolver los problemas de espacio en disco eliminando archivos no utilizados o aumentando la capacidad del disco en el servidor/contenedor.

**Identifica Problemas de Permisos y Autenticaci√≥n**

Los problemas de permisos y autenticaci√≥n a menudo resultan en fallos en la canalizaci√≥n debido a la falta de acceso a los recursos o servicios necesarios. Estos problemas pueden ocurrir cuando intentas acceder a bases de datos, desplegar servicios en la nube, o autenticar API de terceros.

Hay algunos indicadores clave en los registros que puedes buscar:

#### 1\. Fallos de Autenticaci√≥n:

Busca mensajes relacionados con inicios de sesi√≥n fallidos, credenciales incorrectas, o tokens inv√°lidos.

Aqu√≠ tienes un ejemplo:

```
Autenticaci√≥n fallida para el usuario 'admin'
```

```
Token de API proporcionado inv√°lido.
```

#### 2\. Permiso Denegado:

Los registros pueden indicar que la canalizaci√≥n CI/CD carece de los permisos para realizar una cierta acci√≥n.

Aqu√≠ tienes un ejemplo:

```
Acceso denegado para /ruta/a/objetivo/de/despliegue
```

```
Solicitud no autorizada al servicio en la nube.
```

**C√≥mo resolver estos errores**:

-   **Credenciales**: Aseg√∫rate de que las credenciales (claves de API, tokens de acceso, claves SSH) utilizadas en la canalizaci√≥n est√©n actualizadas y configuradas correctamente.
    
-   **Permisos**: Revisa y actualiza la configuraci√≥n de control de acceso basado en roles (RBAC) para la cuenta de servicio que ejecuta la canalizaci√≥n para asegurarte de que tenga los permisos necesarios.
    
-   **Gesti√≥n de Secretos**: Usa herramientas como Vault, AWS Secrets Manager, o Azure Key Vault para gestionar de manera segura los secretos y credenciales.
    

**Resoluci√≥n de Desv√≠os de Configuraci√≥n entre Entornos**

El desv√≠o de configuraci√≥n ocurre cuando los distintos entornos (como desarrollo, preproducci√≥n, producci√≥n) no est√°n sincronizados. Esto puede llevar a un comportamiento inconsistente durante los despliegues, y a menudo resulta en fallos en un entorno pero no en otros.

Busca estos indicadores clave en los registros:

#### 1\. Desajuste en Variables de Entorno:

Si est√°s usando variables de entorno, revisa si hay discrepancias entre las distintas etapas. Por ejemplo:

```
Variable de entorno DATABASE_URL no encontrada en producci√≥n
```

#### 2\. Versiones de Dependencias:

Las versiones desajustadas de dependencias entre entornos pueden causar problemas inesperados.

Aqu√≠ tienes un ejemplo:

```
Error: Desajuste de versi√≥n de la dependencia 'libxyz' entre entornos
```

#### 3\. Configuraci√≥n de Servicio:

Busca errores relacionados con la configuraci√≥n que pueden no estar presentes en un entorno de desarrollo pero ocurren en producci√≥n.

Aqu√≠ tienes un ejemplo:

```
Error: Configuraci√≥n inv√°lida en 'production-config.yaml'
```

**C√≥mo resolver estos errores**:

-   **Usa Infraestructura como C√≥digo (IaC)**: Herramientas como Terraform, Ansible, o CloudFormation pueden ayudar a asegurar que los entornos se aprovisionen de manera consistente.
    
-   **Gesti√≥n Automatizada de Configuraci√≥n**: Usa pasos en la canalizaci√≥n CI/CD para automatizar la configuraci√≥n de entornos para evitar cambios manuales que puedan causar desv√≠os.
    
-   **Comprobaciones de Consistencia de Entorno**: Implementa verificaciones para comparar configuraciones y dependencias entre entornos antes del despliegue.
    
    -   Ejemplo: Puedes agregar una etapa de pre-despliegue para ejecutar un script que compare variables de entorno, configuraciones, y versiones de dependencias entre preproducci√≥n y producci√≥n.
-   **Herramientas de Gesti√≥n de Configuraci√≥n**: Usa herramientas de gesti√≥n de configuraci√≥n como Chef, Puppet, o SaltStack para mantener configuraciones consistentes entre entornos.
    

### C√≥mo Depurar Problemas de Despliegue Basados en Contenedores

Depurar problemas de despliegue basados en contenedores requiere herramientas y t√©cnicas especializadas para rastrear errores en entornos de contenedores. A continuaci√≥n, se presentan estrategias para recolectar registros, diagnosticar fallos, y usar contenedores ef√≠meros para la investigaci√≥n.

#### Recolecci√≥n y An√°lisis Efectivo de Registros de Contenedores

Los registros de contenedores son esenciales para la soluci√≥n de problemas, y una recolecci√≥n y an√°lisis efectivos pueden acelerar significativamente el proceso de depuraci√≥n.

Aqu√≠ te mostramos c√≥mo puedes recolectar registros de contenedores:

**1\. Registros de Docker:**

Puedes usar el comando `logs` de Docker para ver los registros de un contenedor espec√≠fico:

```
docker logs <nombre_del_contenedor_o_id>
```

Si tu contenedor usa un controlador de registro (como `json-file` o `fluentd`), aseg√∫rate de que los registros se escriban en una ubicaci√≥n accesible.

**2\. Registros de Kubernetes:**

Para contenedores gestionados por Kubernetes, utiliza `kubectl` para acceder a los registros del pod:

```
kubectl logs <nombre_del_pod>
```

Para ver los registros de todos los contenedores en un pod:

```
kubectl logs <nombre_del_pod> --all-containers=true
```

**3\. Agregaci√≥n de Registros:**

Puedes integrarte con sistemas de registro centralizados (como, **Grafana Loki**, **Elastic Stack**). Tambi√©n puedes usar Fluentd o Logstash como remitentes de registros para enviar los registros de contenedores a un backend de registro.


**1\. Filtrar y Buscar Registros:**

Usa `grep` para filtrar registros en busca de mensajes de error espec√≠ficos o patrones:

```
docker logs <nombre_del_contenedor> | grep "ERROR"
```

En Kubernetes, puedes combinar `kubectl` con `grep` u otras herramientas para un filtrado avanzado.

**2\. Contextualizaci√≥n de Registros:**

Incluye metadatos en tus registros (por ejemplo, ID del contenedor, entorno, marcas de tiempo) para facilitar la depuraci√≥n. Aseg√∫rate de que los registros est√©n estructurados en formatos como JSON para permitir una mejor consulta y filtrado.

### C√≥mo Diagnosticar Fallos de Descarga de Imagen y de Conectividad de Red

Las fallas en el despliegue de contenedores a menudo provienen de problemas relacionados con la descarga de im√°genes o la conectividad de red. Aqu√≠ te explicamos c√≥mo solucionar estos problemas:

#### Fallos de Descarga de Imagen:

Hay algunos problemas comunes que podr√≠as ver, como:

-   **Fallos de autenticaci√≥n:** Si el registro de contenedores requiere autenticaci√≥n, aseg√∫rate de que tus credenciales (nombre de usuario/contrase√±a o tokens) sean correctas.
    
-   **Conectividad de red:** Verifica si el contenedor puede acceder al endpoint de registro. A menudo, los cortafuegos o problemas de DNS bloquean la descarga de la imagen.
    
-   **Imagen no encontrada:** Verifica que el nombre de la imagen y la etiqueta sean correctos. Usa `docker pull` para descargar manualmente la imagen y ver si el problema es espec√≠fico del proceso de despliegue.
    

Hay varias formas de diagnosticarlos:

Para **Docker**, usa:

```
docker pull <nombre_de_imagen>
```

Esto mostrar√° el mensaje de error espec√≠fico si la descarga de la imagen falla.

Para **Kubernetes**, revisa los registros de eventos del pod:

```
kubectl describe pod <nombre_del_pod>
```

Busca el estado `Failed` bajo "Events" para obtener informaci√≥n sobre por qu√© fall√≥ la descarga de la imagen (por ejemplo, credenciales incorrectas o etiqueta). Si el problema es con la autenticaci√≥n del registro, configura los **imagePullSecrets** de Kubernetes o las credenciales de Docker para asegurar el acceso correcto.

#### Fallos de Conectividad de Red:

Algunos problemas comunes que puedes encontrar son:

-   **Problemas de resoluci√≥n DNS:** Los contenedores pueden fallar al resolver nombres de host si las configuraciones DNS son incorrectas.
    
-   **Pol√≠ticas de red y reglas de cortafuegos:** Las pol√≠ticas de red o cortafuegos pueden bloquear puertos necesarios.
    
-   **Comunicaci√≥n entre contenedores:** Si los contenedores necesitan comunicarse entre s√≠, aseg√∫rate de que est√©n en la misma red o subred.
    

Otra vez, hay varias formas de diagnosticar estos problemas:

**Para la red de Docker:**

Puedes hacer esto para ver todas las redes de Docker:

```
docker network ls
```

Tambi√©n puedes inspeccionar la red de tu contenedor as√≠:

```
docker network inspect <nombre_de_red>
```

Verifica si el contenedor est√° correctamente conectado a la red y si los puertos necesarios est√°n expuestos.

**Para la red de Kubernetes:**

Puedes usar `kubectl` para verificar las pol√≠ticas de red:

```
kubectl get networkpolicies
```

Tambi√©n puedes verificar la configuraci√≥n de red del pod as√≠:

```
kubectl describe pod <nombre_del_pod> | grep -i "Network"
```

**Probar la Conectividad Dentro de los Contenedores:**

Para Docker, accede al contenedor y prueba:

```
docker exec -it <id_del_contenedor> /bin/bash
ping <nombre_de_host_o_ip>
curl http://<direccion_servicio>:<puerto>
```

En Kubernetes, usa `kubectl exec` para acceder al pod y probar la conectividad:

```
kubectl exec -it <nombre_del_pod> -- /bin/bash
```

### C√≥mo Usar Contenedores de Depuraci√≥n Ef√≠meros para Investigaci√≥n

Los contenedores de depuraci√≥n ef√≠meros son contenedores de corta duraci√≥n que ayudan a investigar problemas en un entorno en ejecuci√≥n sin alterar el contenedor de la aplicaci√≥n principal.

#### ¬øQu√© son los Contenedores de Depuraci√≥n Ef√≠meros?

Los contenedores de depuraci√≥n ef√≠meros te permiten ejecutar comandos de diagn√≥stico (como acceso a la shell, `ping` o `curl`) en el mismo entorno de red que el contenedor de aplicaci√≥n que falla, sin modificar la aplicaci√≥n en s√≠.

#### C√≥mo Configurar Contenedores Ef√≠meros en Docker:

**1\. Usa el** comando `docker run`:

Puedes crear un nuevo contenedor para depuraci√≥n ejecutando un contenedor con la misma configuraci√≥n de red que el contenedor que falla:

```
docker run -it --network container:<nombre_o_id_del_contenedor> --entrypoint /bin/bash <imagen_de_depuracion>
```

Este comando ejecuta una shell interactiva dentro del contenedor de depuraci√≥n usando la misma red que el contenedor de destino.

#### Contenedores Ef√≠meros en Kubernetes:

Kubernetes te permite inyectar un contenedor de depuraci√≥n ef√≠mero en un pod en ejecuci√≥n. Puedes agregar un contenedor de depuraci√≥n temporal a tu pod usando el siguiente comando:

```
kubectl debug <nombre_del_pod> -it --image=<imagen_de_depuracion> --target=<nombre_del_contenedor>
```

Este comando ejecutar√° un nuevo contenedor en el mismo pod que el contenedor de destino, permiti√©ndote ejecutar comandos de diagn√≥stico.

Los casos de uso incluyen investigar sistemas de archivos, ejecutar diagn√≥sticos de red, revisar archivos de configuraci√≥n, y as√≠ sucesivamente.

Estos contenedores de depuraci√≥n est√°n dise√±ados para ser temporales y pueden ser descartados una vez que se resuelva el problema.

## C√≥mo Implementar T√©cnicas de Depuraci√≥n Avanzadas

Esta secci√≥n cubre m√©todos avanzados para diagnosticar problemas complejos del flujo de trabajo de CI/CD que un an√°lisis est√°ndar de registros podr√≠a pasar por alto. Exploraremos la trazabilidad distribuida para rastrear solicitudes a trav√©s de m√∫ltiples servicios y combinaremos trazas con registros y m√©tricas para obtener una visi√≥n m√°s profunda.

Estas t√©cnicas est√°n dise√±adas para funcionar dentro de las limitaciones presupuestarias, asegurando una depuraci√≥n efectiva para tus flujos de trabajo de CI/CD.

El seguimiento distribuido te permite monitorear la trayectoria de una solicitud a trav√©s de varios servicios en tu pipeline CI/CD, como desde un paso de construcci√≥n hasta un despliegue, identificando retrasos o fallas. Elegir un backend de seguimiento implica seleccionar una herramienta para almacenar y analizar estos datos de seguimiento. A continuaci√≥n, comparamos Jaeger, Tempo y soluciones alojadas para el seguimiento distribuido.

| **Herramienta** | **Uso de Recursos** | **Complejidad de Configuraci√≥n** | **Mejor Para** | **Encaje con CI/CD** |
| --- | --- | --- | --- | --- |
| **Jaeger** | Bajo | F√°cil (basado en Docker) | Equipos peque√±os, configuraciones locales | Pipelines simples, vistas de seguimiento r√°pidas |
| **Tempo** | Bajo | Moderado (integraci√≥n con Grafana) | Usuarios de Grafana, correlaci√≥n log/m√©trica | Pipelines complejos, observabilidad unificada |
| **Alojado (ej., Lightstep)** | Variable (basado en la nube) | F√°cil (gestionado) | Equipos con presupuesto para servicios en la nube | Seguimiento escalable, de grado de producci√≥n |

Cu√°ndo elegir cada uno:

- **Jaeger**: Ideal para configuraciones de seguimiento r√°pidas y locales con sobrecarga m√≠nima.
- **Tempo**: Mejor para equipos que ya utilizan Grafana Loki/Prometheus para observabilidad unificada.
- **Soluciones Alojadas**: Adecuadas para pipelines a gran escala que necesitan escalabilidad gestionada.

### C√≥mo Configurar el Seguimiento Distribuido con un Presupuesto

El seguimiento distribuido es crucial para depurar y observar operaciones complejas y de m√∫ltiples pasos a trav√©s de servicios. Te permite seguir las solicitudes a medida que se propagan a trav√©s de diferentes servicios y componentes de tu pipeline. Implementarlo con un presupuesto a√∫n puede proporcionar informaci√≥n valiosa.

#### C√≥mo Usar OpenTelemetry con Backends Gratuitos

[OpenTelemetry][26] es un marco de c√≥digo abierto que te permite recopilar, procesar y exportar datos de telemetr√≠a como trazas y m√©tricas. Admite m√∫ltiples backends, y nos centraremos en utilizar backends gratuitos y econ√≥micos para el almacenamiento y an√°lisis de trazas.

**1\. Instalar OpenTelemetry Collector:**

OpenTelemetry proporciona un agente (colector) que recopila trazas y m√©tricas de tu aplicaci√≥n y las env√≠a a un backend.

Para instalar el OpenTelemetry Collector, descarga el binario para tu sistema operativo o utiliza Docker para desplegarlo:

```
docker pull otel/opentelemetry-collector:latest
```

Luego ejecuta el OpenTelemetry Collector en Docker con un archivo de configuraci√≥n:

```
docker run -d --name opentelemetry-collector -p 55680:55680 -p 14250:14250 otel/opentelemetry-collector
```

**2\. Configurar OpenTelemetry para Exportar a Backends Gratuitos:**

Hay algunos backends gratuitos populares que puedes usar para el seguimiento distribuido, como Jaeger y Prometheus + Tempo. Veamos c√≥mo usar ambos aqu√≠.

Comenzaremos con **Jaeger**, un backend de seguimiento de c√≥digo abierto. Es altamente escalable y funciona bien con OpenTelemetry.

Puedes usar la versi√≥n de Docker para un despliegue f√°cil:

```
docker run -d --name jaeger -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 -p 5775:5775 -p 6831:6831/udp -p 6832:6832/udp -p 5778:5778 -p 16686:16686 -p 14250:14250 -p 14268:14268 -p 14250:14250 -p 9431:9431 jaegertracing/all-in-one:1.30
```

Alternativamente, puedes usar servicios alojados como **Lightstep**, **AWS X-Ray** o **Honeycomb** para entornos nativos en la nube.

Ahora veamos c√≥mo utilizar **Prometheus** + **Tempo** para la correlaci√≥n de logs y m√©tricas.

Tempo es un backend de seguimiento distribuido construido por Grafana que se integra bien con otras herramientas de Grafana (Loki y Prometheus).

Puedes instalar Tempo usando Docker:

```
docker run -d --name tempo -p 14268:14268 grafana/tempo:latest
```

**3\. Instrumentar tu C√≥digo con el SDK de OpenTelemetry:**

Para aplicaciones en Python/Node.js/Java/Go, puedes instalar el SDK adecuado de OpenTelemetry y comenzar a trazar.

Aqu√≠ hay un ejemplo en Python:

```
pip install opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation
```

Y un ejemplo en Node.js:

```
npm install @opentelemetry/api @opentelemetry/sdk-node @opentelemetry/instrumentation
```

Y uno en Java:

```
<dependency>
    <groupId>io.opentelemetry</groupId>
    <artifactId>opentelemetry-api</artifactId>
    <version>1.0.0</version>
</dependency>
```

Despu√©s de la instalaci√≥n, puedes usar el SDK de OpenTelemetry para instrumentar la aplicaci√≥n y comenzar a recopilar trazas para solicitudes HTTP, consultas de base de datos y otras interacciones del pipeline.

**4\. Enviar Datos al Collector:**

Puedes configurar el SDK para enviar datos de traza a tu OpenTelemetry Collector, que luego los reenviar√° a tu backend (Jaeger, Tempo, etc.). Aqu√≠ hay un ejemplo para Python:

```
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchExportSpanProcessor

trace.set_tracer_provider(TracerProvider())
exporter = OTLPSpanExporter(endpoint="http://localhost:55680")
processor = BatchExportSpanProcessor(exporter)
trace.get_tracer_provider().add_span_processor(processor)
```

Si las trazas no aparecen, pueden estar ocurriendo varios problemas:

1. **Fallo al iniciar el Collector**: Revisa los logs con `docker logs otel-collector`. Busca errores como "conflicto de puerto" o "configuraci√≥n inv√°lida".
    
    - Soluci√≥n: Cambiar los puertos (por ejemplo, `55681:55680`) o verificar el archivo de configuraci√≥n.
2. **No hay trazas en Jaeger**: Aseg√∫rate de que el collector est√© enviando datos a Jaeger (`http://localhost:14250`). Prueba con `curl http://localhost:55680`.
    
    - Soluci√≥n: Actualiza el endpoint del exportador en tu configuraci√≥n del SDK.
3. **Restricciones de recursos**: Monitorea el uso con `docker stats`.
    
    - Soluci√≥n: Asigna al menos 2GB de RAM y 10GB de espacio en disco para el collector y el backend.

Combinar trazas con registros y m√©tricas proporciona una visi√≥n hol√≠stica de las operaciones de su pipeline, permiti√©ndole identificar la causa ra√≠z de los problemas de manera m√°s efectiva.

OpenTelemetry y Grafana le permiten vincular trazas, registros y m√©tricas en una vista unificada.

Veamos c√≥mo puede hacer esto ahora.

**1\. Vincular registros y trazas usando IDs de correlaci√≥n:**

Cuando genere registros, incluya IDs de trazas y span en las entradas de registro. Esto le permite correlacionar registros con solicitudes de trazas espec√≠ficas.

Aqu√≠ hay un ejemplo:

```
{
  "timestamp": "2025-05-10T12:00:00Z",
  "level": "error",
  "mensaje": "Fallo de compilaci√≥n",
  "trace_id": "1234567890abcdef",
  "span_id": "0987654321abcdef"
}
```

**2\. Integrar registros (Loki) con trazas (Jaeger/Tempo) en Grafana:**

Grafana puede integrar trazas de Jaeger o Tempo y correlacionarlas con los registros de Loki.

Para hacer esto:

1.  **Configure Loki y Tempo en Grafana.**
    
2.  En la vista Explore de Grafana, puede buscar registros y trazas lado a lado.
    
3.  Cree paneles que muestren m√©tricas, registros y trazas para una visi√≥n completa del flujo de solicitudes.
    

**3\. Usar m√©tricas de Prometheus con trazas:**

Prometheus proporciona m√©tricas que pueden correlacionarse con trazas. Por ejemplo, puede usar **ejemplares** en Prometheus para vincular datos de m√©tricas espec√≠ficas con datos de trazas.

**Ejemplo:** Si tiene una alta tasa de errores en su paso de construcci√≥n, puede correlacionar esto con datos de trazas para identificar qu√© solicitudes fallaron.

#### Creaci√≥n de visualizaciones de trazas para operaciones complejas de pipeline

Puede visualizar trazas con Jaeger o Tempo.

**Para hacer esto en Jaeger:**

Una vez que sus trazas est√©n en Jaeger, puede acceder a la interfaz de usuario de Jaeger ([`http://localhost:16686`][27] por defecto) y usar la funcionalidad de b√∫squeda para explorar trazas seg√∫n el nombre del servicio, el ID de trazas o operaciones espec√≠ficas.

Jaeger le permite crear paneles personalizados para visualizar la latencia, el rendimiento y los errores de solicitudes a trav√©s de los servicios.

**Para hacer esto en Tempo (Integraci√≥n con Grafana):**

Tempo se integra con Grafana, donde puede crear paneles que visualicen datos de trazas de su pipeline.

**Cree un panel de Grafana:**

1.  Agregue Tempo como fuente de datos en Grafana.
    
2.  Use el panel "Trace" para consultar y visualizar trazas.
    
3.  Combine visualizaciones de trazas con m√©tricas (de Prometheus) y registros (de Loki) para obtener una vista unificada de su pipeline.
    

Un panel de visualizaci√≥n de trazas t√≠pico podr√≠a mostrar la duraci√≥n de cada paso en su pipeline (construir, probar, desplegar) y resaltar d√≥nde ocurren retrasos o errores, como consultas de bases de datos lentas o pruebas inestables.

**Resoluci√≥n de problemas de configuraci√≥n de Tempo**

Si Tempo no logra recolectar o mostrar trazas:

1.  **El contenedor no se inicia**: Revise los registros con `docker logs tempo`. Busque errores como "puerto ya en uso" (por ejemplo, 14268) o "almacenamiento de respaldo no disponible".
    
    -   Soluci√≥n: Cambie los puertos en el comando de Docker (por ejemplo, `-p 14269:14268`) o aseg√∫rese de que el directorio de almacenamiento (por ejemplo, `/tmp/tempo`) exista y sea escribible.
2.  **No hay trazas en Tempo**: Verifique que el OpenTelemetry Collector est√© enviando trazas al endpoint de Tempo (`http://localhost:14268`). Pruebe la conectividad con `curl http://localhost:14268`.
    
    -   Soluci√≥n: Actualice la configuraci√≥n del exportador del colector para apuntar al endpoint correcto de Tempo y aseg√∫rese de que no haya firewalls bloqueando la conexi√≥n.
3.  **Restricciones de recursos**: Monitoree el uso con `docker stats` o `top` en el host.
    
    -   Soluci√≥n: Asigne al menos 2GB de RAM y 10GB de espacio en disco para Tempo, ya que los datos de trazas pueden crecer r√°pidamente con pipelines de alto volumen.

![Gr√°fico de barras que muestra la latencia de trazas del pipeline CI/CD para mayo de 2025. Se muestran tres etapas del pipeline: etapa de construcci√≥n (barra azul) muestra aproximadamente 1,200ms de latencia, etapa de prueba (barra amarilla) muestra aproximadamente 800ms de latencia, y etapa de despliegue (barra roja) muestra aproximadamente 1,500ms de latencia. La etapa de despliegue tiene la mayor latencia, seguida por construcci√≥n, luego prueba.](https://cdn.hashnode.com/res/hashnode/image/upload/v1748226837500/c9865f8c-f737-49a5-a346-a56f4fac37fd.png)

Este gr√°fico de barras muestra la latencia promedio (en milisegundos) para etapas clave de un pipeline CI/CD en mayo de 2025. La etapa de construcci√≥n promedia alrededor de 1,200 ms (azul), la etapa de prueba alrededor de 800 ms (amarillo), y la etapa de despliegue alrededor de 1,500 ms (rosa), resaltando que el despliegue es el paso m√°s intensivo en tiempo.

## C√≥mo construir paneles de depuraci√≥n comprensivos

Esta secci√≥n explica c√≥mo crear paneles de Grafana para resolver problemas de pipelines CI/CD de manera efectiva. Nos centraremos en configurar visualizaciones para m√©tricas clave, registros y recursos del sistema para identificar problemas como fallos de construcci√≥n o cuellos de botella de recursos, utilizando herramientas econ√≥micas para mantener su pila de observabilidad eficiente y accionable.

### Dise√±ar paneles de Grafana espec√≠ficamente para la soluci√≥n de problemas

#### Paso 1: Comprenda las m√©tricas clave y registros a monitorear

Al dise√±ar un panel de Grafana para depuraci√≥n, debe enfocarse en m√©tricas y registros que ayuden a identificar problemas en el pipeline. Estos podr√≠an incluir:

-   **Fallos de construcci√≥n**: Errores durante los procesos de construcci√≥n (compilaci√≥n, fallos de pruebas).
    
-   **Fallos de despliegue**: Problemas en el despliegue, como trabajos fallidos, limitaciones de recursos o configuraciones err√≥neas.
    
-   **Registros de contenedores**: Informaci√≥n sobre el estado del contenedor y registros (si utiliza contenedores en su pipeline).
    
-   **Uso de recursos del sistema**: CPU, memoria y uso de disco que pueden llevar a cuellos de botella en el rendimiento.
    
-   **M√©tricas espec√≠ficas de CI/CD**: N√∫mero de ejecuciones exitosas versus fallidas del pipeline, duraci√≥n del trabajo, tiempos de cola de trabajo.
    



Para comenzar a construir el panel, necesitar√°s configurar tus fuentes de datos en Grafana. Primero, conecta tu instancia de Prometheus para recopilar m√©tricas. Para hacer esto, ve a `Configuraci√≥n` > `Fuentes de Datos` en Grafana. Luego solo a√±ade `Prometheus` como fuente de datos e ingresa la URL (por ejemplo, [`http://localhost:9090`][28]).

A continuaci√≥n, necesitas conectar tu instancia de Loki para registros. As√≠ que adelante, a√±ade `Loki` como fuente de datos especificando la URL (por ejemplo, [`http://localhost:3100`][29]).

Ten en cuenta que si est√°s utilizando otras fuentes como InfluxDB o Elasticsearch, necesitar√°s asegurarte de que est√©n correctamente conectadas como fuentes de datos.

#### Paso 3: Crear Paneles y Visualizaciones

Ahora que tus fuentes de datos est√°n conectadas, puedes comenzar a construir tu panel con los siguientes paneles:

-   **Panel de Estado de Construcci√≥n:**
    
    -   Crea un **panel de estad√≠sticas** o **panel de medidor** para mostrar la proporci√≥n de √©xito/fracaso de las ejecuciones del pipeline.
        
    -   Consulta en Prometheus o Loki para obtener datos como el estado de la construcci√≥n (√©xito o fracaso), n√∫mero de errores y duraci√≥n de los trabajos.
        
-   **Panel de Desglose de Errores:**
    
    -   Usa un **gr√°fico circular** para visualizar los tipos de errores (por ejemplo, fallos de construcci√≥n, despliegue o recursos del sistema).
        
    -   Consulta los registros en Loki para desglosar los tipos de errores basados en la herramienta de CI (por ejemplo, Jenkins, GitHub Actions).
        
-   **Panel de Utilizaci√≥n de Recursos:**
    
    -   Usa **gr√°ficos de series temporales** para monitorear el uso de CPU, memoria y disco a lo largo del tiempo, especialmente para construcciones o despliegues que requieren muchos recursos.
-   **Panel de Duraci√≥n de Trabajos:**
    
    -   Usa **gr√°ficos de barras** o **gr√°ficos de l√≠neas** para rastrear la duraci√≥n promedio de los trabajos a lo largo del tiempo. Establece umbrales para se√±ales de advertencia si un trabajo tarda m√°s de lo esperado.

#### Solucionar Problemas del Panel de Grafana

Si los paneles de Grafana no muestran datos o muestran errores, podr√≠as tener uno de estos problemas:

1.  **Fuentes de datos faltantes**: Si no aparecen m√©tricas, registros o trazas, verifica las conexiones de fuentes de datos en Grafana (por ejemplo, Prometheus, Loki, Tempo). Revisa en Configuraci√≥n > Fuentes de Datos.
    
    -   Soluci√≥n: Aseg√∫rate de que las URLs de las fuentes de datos sean correctas (por ejemplo, `http://localhost:9090` para Prometheus) y prueba la conexi√≥n. Vuelve a agregar la fuente de datos si es necesario.
2.  **IDs de traza incorrectos**: Si las visualizaciones de trazas (por ejemplo, paneles de Tempo) no muestran datos, confirma que los IDs de traza en los registros coincidan con aquellos en Tempo. Usa una consulta como `{job="ci_cd"} | json | trace_id="1234567890abcdef"` en Loki para revisar.
    
    -   Soluci√≥n: Aseg√∫rate de que los registros de tu aplicaci√≥n incluyan IDs de traza y secci√≥n, y verifica que el SDK de OpenTelemetry est√© correctamente instrumentado para enviar trazas a Tempo.
3.  **Restricciones de recursos**: Monitorea el uso de recursos de Grafana con `docker stats` si se ejecuta en un contenedor, o `top` en el host.
    
    -   Soluci√≥n: Asigna al menos 4GB de RAM y 10GB de espacio en disco para Grafana, especialmente al renderizar paneles complejos con m√∫ltiples fuentes de datos.

### C√≥mo Configurar Rutas de Exploraci√≥n Detallada Desde Vistas Generales a Detalladas

#### Paso 1: Crear Panel de Vista General

En la parte superior del panel, incluye un panel de vista general que resuma el estado general del pipeline. Esto podr√≠a ser:

-   **Recuento de √âxito/Fracaso**: Un panel de estad√≠sticas simple que muestre el recuento de ejecuciones exitosas vs. fallidas.
    
-   **Estado de la Salud del Pipeline**: Muestra una verificaci√≥n de salud general de tu pipeline usando indicadores codificados por color (verde para saludable, rojo para problemas).
    

#### Paso 2: Configurar Enlaces de Exploraci√≥n Detallada

Para permitir a los usuarios explorar desde informaci√≥n a alto nivel a vistas detalladas:

**1\. Enlace a informaci√≥n detallada de construcci√≥n**:

Puedes crear un gr√°fico de series temporales que muestre las duraciones de los trabajos de construcci√≥n. A√±ade un enlace a una vista de registro detallada al hacer clic en un trabajo fallido.

Por ejemplo, al hacer clic en una construcci√≥n fallida, puedes enlazar a un panel detallado o un panel separado que muestre los registros y mensajes de error relacionados con esa ejecuci√≥n espec√≠fica.

**2\. Enlace a Registros en Loki**:

Puedes usar consultas **LogQL de Loki** para configurar un camino de exploraci√≥n detallada. Cuando los usuarios hagan clic en un tipo de error o un nombre de trabajo espec√≠fico, deber√≠a filtrar autom√°ticamente los registros para ese trabajo o tipo de error.

Puedes configurar interacciones de exploraci√≥n detallada usando Enlaces de Panel en Grafana. En la configuraci√≥n del panel, bajo `Links`, especifica el enlace a otro panel que muestre registros detallados filtrados por el nombre del trabajo o tipo de fallo.

#### Paso 3: Implementar Filtros de Rango de Tiempo

Para mejorar la funcionalidad de exploraci√≥n detallada, puedes a√±adir un **filtro de rango de tiempo** para permitir a los usuarios ajustar la ventana de tiempo para ambos registros y m√©tricas. Esto les permite hacer zoom en un intervalo de tiempo espec√≠fico donde ocurrieron fallas.

### C√≥mo Crear Paneles Compartidos para Soluci√≥n de Problemas en Equipo

#### Paso 1: Compartir Tu Panel

Una vez que tu panel est√© dise√±ado, puedes compartirlo con tu equipo para la soluci√≥n de problemas de manera colaborativa:

Primero, querr√°s asegurarte de que los permisos correctos est√©n configurados para tu equipo. Puedes definir roles espec√≠ficos en Grafana con acceso al panel. Ve a `Configuraci√≥n del Panel` > `Permisos`, y otorga acceso de visualizaci√≥n o edici√≥n a usuarios o equipos.

A continuaci√≥n, puedes compartir directamente un enlace al panel con los miembros de tu equipo. Usa la opci√≥n `Compartir` en la esquina superior derecha del panel, que proporciona una URL directa y tambi√©n opciones para incrustar el panel en otras herramientas (por ejemplo, Slack, correo electr√≥nico).

#### Paso 2: Configurar alertas

Para asegurarse de que su equipo est√© notificado de cualquier fallo en la canalizaci√≥n, puede configurar **reglas de alerta**. Hay algunas importantes que querr√° configurar.

Primero, cree alertas para problemas cr√≠ticos, como cuando una canalizaci√≥n falla o excede el uso de recursos esperado. Esto podr√≠a ser para cosas como el tiempo de construcci√≥n que excede un umbral o el fallo de una etapa de implementaci√≥n.

Grafana puede enviar alertas a trav√©s de varios canales como Slack, correo electr√≥nico o webhook.

Tambi√©n puede integrar sus paneles con herramientas como Slack o Teams para notificaciones en tiempo real y colaboraci√≥n. Configure mensajes autom√°ticos para su equipo cuando el panel indique un problema.

### **C√≥mo crear herramientas de diagn√≥stico automatizadas**

#### Crear scripts que recopilen registros relevantes durante fallos

Para automatizar la recopilaci√≥n de registros durante fallos, necesita scripts que puedan capturar registros de diferentes etapas y servicios de CI/CD tan pronto como se detecte un fallo. Aqu√≠ est√°n los pasos que puede seguir para hacerlo:

**1\. Escribir un script de detecci√≥n de fallos:**

Puede aprovechar los c√≥digos de estado de salida de sus herramientas de CI/CD para detectar fallos. Por ejemplo, en GitLab CI/CD o GitHub Actions, puede verificar si el √∫ltimo comando fall√≥ inspeccionando `$?` en sistemas basados en Unix.

```
# Ejemplo para GitLab CI/CD
if [ $? -ne 0 ]; entonces
    echo "Fallo detectado, recopilando registros..."
    # Llamada al script de recopilaci√≥n de registros personalizado
    ./collect_logs.sh
fi
```

**2\. Script de recopilaci√≥n de registros (collect\_**[**logs.sh**][30]**):**

El script debe recopilar registros relevantes, m√©tricas del sistema e informaci√≥n de trazas. Por ejemplo:

```
#!/bin/bash
LOG_DIR="/ruta/a/registros"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="${LOG_DIR}/backup/${TIMESTAMP}"
mkdir -p $BACKUP_DIR

# Recopilar registros de agentes de CI/CD, contenedores o registros del sistema
cp /var/log/ci_cd/*.log $BACKUP_DIR/
cp /ruta/a/docker_logs/*.log $BACKUP_DIR/
# Recopilar m√©tricas o trazas de sistemas de monitoreo si es necesario
```

**3\. Usar artefactos de CI/CD:**

Para plataformas como GitLab, GitHub Actions o Jenkins, puede cargar registros como artefactos para una investigaci√≥n m√°s detallada. Configure estas plataformas para guardar registros en caso de fallo.

Aqu√≠ hay un ejemplo para GitHub Actions:

```
steps:
  - name: Ejecutar Pruebas
    run: |
      npm run test
  - name: Cargar registros si la prueba falla
    if: failure()
    uses: actions/upload-artifact@v2
    with:
      name: test-logs
      path: /ruta/a/prueba/registros
```

**4\. Registro centralizado:**

En lugar de recopilar registros manualmente, puede centralizar el almacenamiento de registros utilizando sistemas de registro como Grafana Loki, ELK stack o incluso soluciones basadas en la nube. Esto asegurar√° que los registros sean accesibles incluso si son sobrescritos o perdidos en sistemas individuales.

### C√≥mo implementar an√°lisis autom√°tico de patrones comunes de errores

Una vez que los registros son recolectados, puede automatizar el proceso de an√°lisis definiendo patrones comunes de errores y busc√°ndolos autom√°ticamente en sus registros.

#### Paso 1: Definir patrones de error:

Establezca firmas de error o patrones que sean comunes en su proceso de CI/CD, como construcciones fallidas debido a dependencias faltantes, problemas de permisos o interrupciones de red.

Puede usar expresiones regulares para capturar estos patrones. Aqu√≠ hay un ejemplo: definir un regex para patrones de pruebas fallidas:

```
TEST_FAILURE_REGEX=".*FAILURE.*"
```

#### Paso 2: Crear script de an√°lisis de registros:

A continuaci√≥n, puede escribir un script que escanee registros en busca de estos patrones comunes. Entonces el script podr√≠a categorizar o marcar errores.

Aqu√≠ hay un ejemplo usando `grep` para detectar patrones de fallo:

```
#!/bin/bash
LOG_DIR="/ruta/a/registros"
ERROR_LOG="${LOG_DIR}/error_patterns.log"
touch $ERROR_LOG

# Definir patrones de error a buscar
ERROR_PATTERNS=("FAILURE" "ERROR" "TIMEOUT")

for PATTERN in "${ERROR_PATTERNS[@]}"; do
    grep -i $PATTERN $LOG_DIR/*.log >> $ERROR_LOG
done

if [ -s $ERROR_LOG ]; then
    echo "Patrones de error encontrados, revise el archivo de registro."
fi
```

#### Paso 3: Automatizar alertas:

Una vez detectado un patr√≥n de error, puede integrar el script de an√°lisis de registro con su sistema de alertas (por ejemplo, enviando un correo electr√≥nico o una notificaci√≥n de Slack).

Aqu√≠ hay un ejemplo de env√≠o de una notificaci√≥n de Slack:

```
if [ -s $ERROR_LOG ]; then
    curl -X POST -H 'Content-type: application/json' \
         --data '{"text":"Error detectado en la canalizaci√≥n CI. Revise el registro de errores."}' \
         https://hooks.slack.com/services/TU_URL_DE_SLACK_WEBHOOK
fi
```

#### Paso 4: Usar herramientas de observabilidad para el reconocimiento de patrones:

Aproveche las herramientas de observabilidad (Grafana Loki, Prometheus) que soportan consultas y visualizaci√≥n de registros. Puede crear paneles que detecten autom√°ticamente anomal√≠as como altas tasas de fallos o errores recurrentes.

Ejemplo: Configure un panel de Grafana con reglas de alerta basadas en la frecuencia de registros.

### C√≥mo crear canalizaciones autocurativas basadas en problemas conocidos

Las canalizaciones autocurativas pueden abordar autom√°ticamente problemas cuando se detectan ejecutando acciones correctivas predefinidas. Veamos c√≥mo puede configurarlo.

#### Paso 1: Definir fallos comunes y soluciones:

Identifique problemas recurrentes (por ejemplo, problemas de dependencias, tiempos de espera de construcci√≥n, pruebas inconsistentes) que ocurren en su canalizaci√≥n. Luego, defina acciones autocurativas para mitigar estos problemas.

```
trabajos:
  construir:
    se ejecuta en: ubuntu-latest
    pasos:
      - nombre: Ejecutar Pruebas
        ejecutar: |
          npm run test
      - nombre: Reintentar Pruebas si Fallaron
        si: failure() && (steps.tests.outcome == 'failure')
        ejecutar: |
          echo "Reintentando las pruebas..."
          npm run test
```

#### Paso 2: Recuperaciones Autom√°ticas:

Configure un proceso de recuperaci√≥n para implementaciones fallidas. Por ejemplo, si una implementaci√≥n en producci√≥n falla, la canalizaci√≥n puede revertir autom√°ticamente a la √∫ltima compilaci√≥n exitosa.

Ejemplo en GitLab CI/CD:

```
deploy_production:
  script:
    - ./deploy.sh
  when: on_failure
  retry: 3
```

#### Paso 3: Construir L√≥gica de Autocorrecci√≥n Usando Mecanismos de Reintento:

Implemente la l√≥gica de reintento para problemas transitorios (como fallos en la red) que a menudo causan fallos.

Ejemplo de reintentar un paso en GitHub Actions:

```
steps:
  - nombre: Reintentar Despliegue
    ejecutar: |
      attempts=0
      max_attempts=3
      until [ $attempts -ge $max_attempts ]
      do
        deploy_script && break
        attempts=$((attempts+1))
        echo "Intento $attempts fallido. Reintentando..."
        sleep 5
      done
```

#### Paso 4: Automatizar Acciones Correctivas para Problemas de Dependencias:

Configure correcciones autom√°ticas para fallos relacionados con dependencias, como limpiar cach√©s o reinstalar dependencias:

```
if [[ $(cat error.log) =~ "dependency not found" ]]; then
    echo "Problema de dependencia detectado, reinstalando dependencias..."
    npm install
fi
```

#### Paso 5: Integrarse con Servicios de Autocorrecci√≥n:

Para una autocorrecci√≥n m√°s compleja, puede integrar herramientas como Ansible, Puppet, o incluso crear scripts personalizados que solucionen autom√°ticamente problemas comunes de configuraci√≥n.

## C√≥mo Conducir Autopsias Efectivas Usando Registros

Los registros a menudo son el recurso m√°s valioso al reconstruir lo que sali√≥ mal en una canalizaci√≥n de CI/CD. Realizar autopsias efectivas con datos de registros permite a los equipos extraer cronolog√≠as claras, identificar causas ra√≠z y definir pasos para prevenir recurrencia, todo basado en evidencia concreta.

### Extraer Cronolog√≠a y Eventos Clave de los Registros

Para comprender con precisi√≥n qu√© ocurri√≥ y cu√°ndo a partir de la informaci√≥n contenida en sus registros, hay un proceso sencillo que puede seguir.

#### Paso 1: Centralizar y Estructurar Registros:

Primero, aseg√∫rese de que los registros de todas las etapas de la canalizaci√≥n (construcci√≥n, prueba, despliegue) se acumulen en un lugar central como Grafana Loki, ELK, u OpenSearch.

Y querr√° usar un formato de registro consistente (como JSON estructurado) que incluya marcas de tiempo, niveles de registro, identificadores de etapas de canalizaci√≥n, e identificadores de correlaci√≥n/solicitud.

#### Paso 2: Construir una Vista Cronol√≥gica:

Puede usar filtros de marcas de tiempo en su interfaz de usuario de registros (por ejemplo, Kibana, Grafana Explore) para aislar registros del marco temporal del incidente.

Busque eventos clave del ciclo de vida, como:

-   Inicio y finalizaci√≥n de pasos de la canalizaci√≥n
    
-   Cambios de estado (por ejemplo, "prueba fallida", "despliegue iniciado", "construcci√≥n en cola")
    
-   Mensajes de error y advertencias
    
-   Eventos de reintento o reinicios inesperados
    

#### Paso 3: Extraer Registros Program√°ticamente (opcional):

Use consultas (LogQL, Elasticsearch DSL) para exportar registros relevantes para an√°lisis o inclusi√≥n en un documento post-mortem.

### C√≥mo Identificar Causas Ra√≠z a Trav√©s del An√°lisis de Registros

Para ir m√°s all√° de los s√≠ntomas y encontrar el problema real, hay varios pasos que puede tomar.

Comience por **buscar la primera falla**. Puede filtrar registros por `level=error` o usar coincidencia de patrones de registro para identificar el signo m√°s _temprano_ de falla. Luego, rastree hacia atr√°s desde la falla usando identificadores de correlaci√≥n o identificadores de paso de canalizaci√≥n.

Segundo, aseg√∫rese de **correlacionar registros a trav√©s de sistemas.** Empareje registros a lo largo de herramientas de CI/CD (como GitHub Actions ‚Üí registros Docker ‚Üí registros Kubernetes). Puede usar identificadores de correlaci√≥n o de trabajos compartidos para agrupar registros de eventos relacionados.

A continuaci√≥n, **preste atenci√≥n a se√±ales intermitentes.** Las advertencias, reintentos, o el rendimiento degradado que preceden a la falla pueden revelar problemas ambientales o relacionados con la configuraci√≥n.

Y finalmente, **verifique las dependencias externas.** Busque errores de tiempo de espera o conexi√≥n que involucren servicios de terceros, APIs en la nube, o componentes de infraestructura interna.

### **C√≥mo Crear Seguimientos Accionables para Prevenir la Recurrencia**

Hay varias cosas que puede hacer para convertir sus hallazgos en mejoras significativas del proceso.

**1\. Documente los Hallazgos Claramente:**

Cree un documento de post-mortem estructurado que incluya:

-   Cronolog√≠a de eventos con fragmentos de registros
    
-   Desencadenante inmediato y causa ra√≠z (basado en registros)
    
-   Resumen del impacto y componentes afectados
    
-   Capturas de pantalla o consultas de registros guardadas para referencia
    

**2\. Defina Acciones Preventivas:**

Ejemplos incluyen:

-   A√±adir alertas faltantes o monitores basados en registros
    
-   Mejorar la verbosidad de los registros o a√±adir metadatos faltantes
    
-   Corregir casos de prueba poco fiables o scripts de despliegue
    
-   Actualizar l√≠mites de infraestructura o estrategias de reintento
    

**3\. Asigne Responsabilidad y Fechas L√≠mite:**

Cada acci√≥n debe tener un due√±o responsable y una fecha de vencimiento. Si es aplicable, cree pruebas automatizadas o medidas de seguridad para detectar problemas similares en el futuro.

**4\. Actualice Libros de Ejecuci√≥n y Libros de Incidentes:**

Agregue patrones de registro, consultas de ejemplo, y resoluciones a la documentaci√≥n compartida. Esto asegura que la pr√≥xima persona que enfrente un problema similar pueda actuar m√°s r√°pido.
```

## **C√≥mo Optimizar el Almacenamiento y Gesti√≥n de Logs**

A medida que tu sistema CI/CD crece, los logs pueden volverse masivos, consumiendo almacenamiento e impactando el rendimiento. Optimizar el almacenamiento de logs te ayuda a asegurarte de que est√°s reteniendo lo que es valioso mientras te mantienes eficiente.

### C√≥mo Implementar Pol√≠ticas de Rotaci√≥n y Retenci√≥n de Logs

Sin rotaci√≥n y retenci√≥n, los logs se acumular√°n interminablemente, llevando al agotamiento del espacio en disco y a un rendimiento deficiente. Puedes ayudar a prevenir esto con **la rotaci√≥n de logs**.

La rotaci√≥n de logs implica crear nuevos archivos de log despu√©s de alcanzar un umbral de tama√±o o tiempo y archivar o eliminar los antiguos.

**Herramienta logrotate de Linux** ‚Äì Configurar `/etc/logrotate.d/<tu-app>`:

```
/var/log/ci_cd/*.log {
    daily
    rotate 7
    compress
    missingok
    notifempty
    create 0640 root adm
}
```

Este ejemplo:

-   Rota diariamente

-   Mantiene 7 d√≠as de logs

-   Comprime los logs antiguos para ahorrar espacio

**Rotaci√≥n de logs en Docker** ‚Äì en `daemon.json`:

```
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "50m",
    "max-file": "5"
  }
}
```

Las pol√≠ticas de retenci√≥n aseguran que los logs antiguos se eliminen autom√°ticamente seg√∫n la edad o el uso del almacenamiento.

Puedes configurar una en Loki as√≠:

```
table_manager:
  retention_deletes_enabled: true
  retention_period: 168h  # 7 d√≠as
```

O en Elasticsearch, utiliza la Gesti√≥n del Ciclo de Vida de √çndices (ILM):

```
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": { "max_age": "3d", "max_size": "1gb" }
        }
      },
      "delete": {
        "min_age": "7d",
        "actions": { "delete": {} }
      }
    }
  }
}
```

### C√≥mo Configurar la Compactaci√≥n de Logs para Almacenamiento a Largo Plazo

La compactaci√≥n reduce la redundancia y mantiene solo la informaci√≥n cr√≠tica de los logs, lo que es ideal para auditor√≠as o an√°lisis a largo plazo.

#### T√©cnicas de Compactaci√≥n:

Existen varias t√©cnicas de compactaci√≥n que puedes probar. Aqu√≠ est√°n un par:

**1\. Loki (modo boltdb-shipper)**:

-   Usa la compactaci√≥n para fusionar trozos de logs y reducir el almacenamiento.

-   Configura en `loki-config.yaml`:

    ```
      schema_config:
        configs:
          - from: 2023-01-01
            store: boltdb-shipper
            object_store: filesystem
            schema: v11
    ```
    
-   Usa una estrategia de baja retenci√≥n y alta compactaci√≥n para logs archivados.

**2\. Elasticsearch**:

-   Utiliza **trabajos de resumen** para reducir la resoluci√≥n de datos antiguos.

-   Almacena logs resumidos, por ejemplo, conteos horarios de eventos similares.

**3\. Archivar en almacenamiento m√°s barato**:

-   Mueve logs de acceso infrecuente a S3 o Azure Blob Storage utilizando reglas de ciclo de vida.

### C√≥mo Equilibrar la Observabilidad con las Restricciones de Recursos

M√°s logs = m√°s observabilidad, pero tambi√©n m√°s coste y sobrecarga. Esto significa que necesitas un equilibrio. Hay varias estrategias que pueden ayudarte a lograr este equilibrio:

1.  **Loguear en niveles apropiados**:
    
    -   Evita logs excesivos de `debug` o `trace` en producci√≥n.
        
    -   Usa niveles `info` y `warn` de manera juiciosa.
        
    -   Solo usa `error` o `critical` para fallas procesables.
        
2.  **Muestreo de logs**:
    
    -   Si las pipelines de alto volumen generan logs repetitivos, habilita el muestreo de logs para reducir duplicados.
        
    -   Herramientas como Vector o Fluent Bit soportan el muestreo.
        
3.  **Filtrar el ruido**:
    
    -   Usa filtros de logs para excluir logs no cr√≠ticos antes de que lleguen al sistema central.
4.  **Separar logs calientes vs. fr√≠os**:
    
    -   **Logs calientes**: datos recientes en tiempo real para depuraci√≥n activa.
        
    -   **Logs fr√≠os**: archivados para cumplimiento, almacenados con menor prioridad de rendimiento/almacenamiento.
        
5.  **Comprimir todo**:
    
    -   Usa compresi√≥n gzip/zstd tanto para logs almacenados como transmitidos.
        
    -   Loki, Elasticsearch y Vector soportan la compresi√≥n de forma nativa.
        

## **Conclusi√≥n**

En este manual, has construido una capa de observabilidad full-stack espec√≠ficamente optimizada para pipelines CI/CD sin romper tu presupuesto de infraestructura. Ahora tienes las herramientas y el conocimiento para:

-   Desplegar Grafana Loki o una alternativa ELK liviana para capturar logs estructurados de todas las partes de tu pipeline.
    
-   Unificar y enriquecer logs a lo largo de herramientas CI/CD (por ejemplo, GitHub Actions, Jenkins, GitLab) usando formatos consistentes y IDs de correlaci√≥n.
    
-   Utilizar consultas de logs poderosas (LogQL, Kibana Query Language) para diagnosticar fallas de compilaci√≥n, pruebas inestables y problemas de despliegue con precisi√≥n.
    
-   Correlacionar logs con m√©tricas y trazas para obtener visibilidad profunda y contextual en el comportamiento de la pipeline.
    
-   Dise√±ar paneles de depuraci√≥n reutilizables y automatizaci√≥n que convierten logs sin procesar en insights y acciones.
    
-   Construir una cultura de conocimientos compartidos de resoluci√≥n de problemas a trav√©s de post-mortems, runbooks y retrospectivas impulsadas por logs.
    

Para ver la capa de observabilidad full-stack en acci√≥n, revisa el c√≥digo y configuraciones completas en mi repositorio de GitHub: [github.com/Emidowojo/CICDObservability][31]. Este repositorio incluye todas las configuraciones para Grafana Loki, OpenTelemetry, Prometheus y m√°s, para que puedas desplegar y explorar toda la pila de observabilidad de la pipeline.

### Pr√≥ximos Pasos para una Implementaci√≥n Avanzada de Observabilidad

1.  **Integrar completamente el rastreo distribuido**: Despliega agentes de OpenTelemetry a trav√©s de tus etapas de construcci√≥n y despliegue. Esto te ayudar√° a visualizar c√≥mo el c√≥digo, las compilaciones y los despliegues fluyen a trav√©s de los sistemas en tiempo real.

2.  **Automatizar scripts de diagn√≥stico y alertas**: Construye scripts para recopilar autom√°ticamente registros y m√©tricas en caso de fallas, y activar alertas cuando se repitan patrones conocidos. Esto permite una detecci√≥n m√°s r√°pida e incluso pipelines de auto-recuperaci√≥n.

3.  **Escalar y reforzar tu infraestructura de registros**: A medida que el uso crece, implementa pol√≠ticas de retenci√≥n, compactaci√≥n y almacenamiento de registros. Explora backends escalables como ClickHouse o almacenamiento de objetos (por ejemplo, S3) para archivos de largo plazo.

4.  **Entrenar a tu equipo en las mejores pr√°cticas de observabilidad**: Comparte tableros, crea documentos de integraci√≥n y programa sesiones de an√°lisis de registros para que el equipo se familiarice con tus herramientas y pr√°cticas.

### üìö Recursos para Aprendizaje Continuo

**Documentaci√≥n y Herramientas Oficiales:**

-   [Documentaci√≥n de Grafana Loki][32]

-   [Gu√≠a de Configuraci√≥n de Promtail][33]

-   [OpenTelemetry][34]

-   [Sintaxis de LogQL][35]

-   [Lenguaje de Consultas de Kibana][36]

-   [Vector (reenv√≠o de registros)][37]

**Comunidades:**

-   [r/devops en Reddit][38]

-   [Slack de CNCF ‚Äì canal #observability][39]

-   [Mejores Pr√°cticas de Gesti√≥n de Registros en Stack Overflow][40]

Al invertir en observabilidad de manera temprana y cuidadosa, no solo reduces el tiempo para detectar y resolver problemas, sino que tambi√©n construyes un proceso de entrega m√°s resiliente, predecible y transparente para todo tu equipo de ingenier√≠a.

Espero que esto te sea √∫til alg√∫n d√≠a. Si llegaste al final de este manual, ¬°gracias por leer! Puedes conectarte conmigo en [LinkedIn][41] o en X [@Emidowojo][42] si deseas mantener el contacto.

[1]: #encabezado-requisitos-previos
[2]: #encabezado-por-qu√©-es-importante-la-observabilidad
[3]: #encabezado-c√≥mo-instalar-y-configurar-grafana-loki-en-infraestructura-econ√≥mica
[4]: #encabezado-c√≥mo-implementar-una-alternativa-a-elk-stack-para-observabilidad-de-pipelines
[5]: #encabezado-c√≥mo-crear-una-estrategia-unificada-de-registros-a-trav√©s-de-componentes-de-pipelines
[6]: #encabezado-c√≥mo-consultar-y-analizar-registros-para-diagn√≥stico-efectivo
[7]: #encabezado-c√≥mo-configurar-m√©tricas-de-prometheus-junto-a-tus-registros
[8]: #encabezado-c√≥mo-crear-tableros-de-grafana-que-combinen-m√©tricas-y-registros
[9]: #encabezado-c√≥mo-usar-ejemplares-para-saltar-de-m√©tricas-a-registros-relevantes
[10]: #encabezado-c√≥mo-diagnosticar-y-solucionar-problemas-comunes-de-cicd
[11]: #encabezado-c√≥mo-implementar-t√©cnicas-avanzadas-de-depuraci√≥n
[12]: #encabezado-c√≥mo-realizar-p√≥stmortems-efectivos-utilizando-registros
[13]: #encabezado-c√≥mo-optimizar-el-almacenamiento-y-gesti√≥n-de-registros
[14]: #encabezado-conclusi√≥n
[15]: https://www.freecodecamp.org/news/what-is-ci-cd/
[16]: https://www.freecodecamp.org/news/helpful-linux-commands-you-should-know/
[17]: https://www.freecodecamp.org/news/the-docker-handbook/
[18]: https://www.freecodecamp.org/news/observability-in-cloud-native-applications/
[19]: https://fluentbit.io/
[20]: https://vector.dev/
[21]: https://www.elastic.co/beats/filebeat
[22]: http://localhost:5601/
[23]: http://Draw.io
[24]: https://plugins.jenkins.io/prometheus/
[25]: https://github.com/prometheus/prometheus
[26]: https://www.freecodecamp.org/news/how-to-use-opentelementry-to-trace-node-js-applications/
[27]: http://localhost:16686
[28]: http://localhost:9090
[29]: http://localhost:3100
[30]: http://logs.sh
[31]: https://github.com/Emidowojo/CICDObservability.git
[32]: https://grafana.com/docs/loki/
[33]: https://grafana.com/docs/loki/latest/clients/promtail/
[34]: https://opentelemetry.io/docs/
[35]: https://grafana.com/docs/loki/latest/logql/
[36]: https://www.elastic.co/guide/en/kibana/current/kuery-query.html
[37]: https://vector.dev/docs/
[38]: https://www.reddit.com/r/devops/
[39]: https://slack.cncf.io/
[40]: https://stackoverflow.com/questions/tagged/logging
[41]: https://www.linkedin.com/in/emidowojo/
[42]: https://x.com/Emidowojo

